# 循环神经网络 --- 从0开始

前面的教程里我们使用的网络都属于**前馈神经网络**。之所以叫前馈，是因为整个网络是一条链（回想下`gluon.nn.Sequential`），每一层的结果都是反馈给下一层。这一节我们介绍**循环神经网络**，这里每一层不仅输出给下一层，同时还输出一个**隐藏状态**，给当前层在处理下一个样本时使用。下图展示这两种网络的区别。

![](../img/rnn_1.png)

循环神经网络的这种结构使得它适合处理前后有依赖关系的样本。我们拿语言模型举个例子来解释这个是怎么工作的。语言模型的任务是给定句子的前*T*个字符，然后预测第*T+1*个字符。假设我们的句子是“你好世界”，使用前馈神经网络来预测的一个做法是，在时间1输入“你”，预测”好“，时间2向同一个网络输入“好”预测“世”。下图左边展示了这个过程。

![](../img/rnn_2.png)

注意到一个问题是，当我们预测“世”的时候只给了“好”这个输入，而完全忽略了“你”。直觉上“你”这个词应该对这次的预测比较重要。虽然这个问题通常可以通过**n-gram**来缓解，就是说预测第*T+1*个字符的时候，我们输入前*n*个字符。如果*n=1*，那就是我们这里用的。我们可以增大*n*来使得输入含有更多信息。但我们不能任意增大*n*，因为这样通常带来模型复杂度的增加从而导致需要大量数据和计算来训练模型。

循环神经网络使用一个隐藏状态来记录前面看到的数据来帮助当前预测。上图右边展示了这个过程。在预测“好”的时候，我们输出一个隐藏状态。我们用这个状态和新的输入“好”来一起预测“世”，然后同时输出一个更新过的隐藏状态。我们希望前面的信息能够保存在这个隐藏状态里，从而提升预测效果。

在更加正式的介绍这个模型前，我们先去弄一个比“你好世界“稍微复杂点的数据。

## 《时间机器》数据集

我们用《时间机器》这本书做数据集主要是因为[古登堡计划](http://www.gutenberg.org)计划使得可以免费下载，而且我们看了太多用莎士比亚作为例子的教程。下面我们读取这个数据并看看前面500个字符（char）是什么样的：

```{.python .input  n=1}
#file = "../data/timemachine.txt"
file = '../data/jaychou_lyrics.txt'


with open(file) as f:
    time_machine = f.read()
print(time_machine[0:500])
```

```{.json .output n=1}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\u4f5c\u8bcd\uff1a\u9ec4\u4fca\u90ce \n\u4f5c\u66f2\uff1a\u5468\u6770\u4f26\n\u7f16\u66f2\uff1a\u9ec4\u96e8\u52db\n\u9418\u9006\u65f6\u9418\u800c\u7ed5 \u6076\u7269\u72f0\u72de\u7684\u503e\u5de2\n\u6211\u8c26\u5351\u5b89\u9759\u7684\u65bc\u57ce\u5821\u4e0b\u7684\u665a\u7977\n\u538b\u6291\u8fdc\u53e4\u6d41\u7a9c\u7684\u86ee\u8352\u6697\u53f7\n\u800c\u7ba1\u98ce\u7434\u952e\u9ad8\u50b2\u7684\u8bf4 \u90a3\u53ea\u662f\u5728\u5f92\u52b3\n\u50b2\u50b2\u50b2\u50b2\u50b2 \u6211\u7684\u4e50\u5668\u5728\u73af\u7ed5\n\u65f6\u4ee3\u65e0\u6cd5\u6dd8\u6c70\u6211\u9738\u6c14\u7684\u7687\u671d\n\u4f60\u65e0\u6cd5\u9884\u8a00 \u56e0\u70ba\u6211\u8d8a\u9669\u7fc5\u8d8a\u8277\n\u6ca1\u6709\u53e5\u70b9 \u8de8\u65f6\u4ee3\u8513\u5ef6\u7ffc\u671d\u5929\n\u6708\u4e0b\u6d6e\u96d5 \u9b54\u9b3c\u7684\u6d45\\\u7b11\n\u72fc\u8fce\u98ce\u568e \u8760\u7fd4\u4f3c\u9ed1\u6f6e\n\u7528\u5b64\u72ec\u53bb\u8c03\u5c0a\u4e25\u7684\u8272\u8c03\n\u6211\u8de8\u8d8a\u8fc7\u65f6\u4ee3 \u5982\u517d\u822c\u7684\u59ff\u6001\n\u7434\u58f0\u5524\u9192\u6c88\u7761\u7684\u8840\u8109\n\u4e0d\u9700\u8981\u88ab\u5d07\u62dc \u5982\u517d\u822c\u7684\u60b2\u54c0\n\u53ea\u70ba\u6c38\u6052\u7684\u4e50\u66f2\u5b58\u5728 \u9192\u8fc7\u6765\n\u9418\u9006\u65f6\u9418\u800c\u7ed5 \u6076\u7269\u72f0\u72de\u7684\u503e\u5de2\n\u6211\u8c26\u5351\u5b89\u9759\u7684\u65bc\u57ce\u5821\u4e0b\u7684\u665a\u7977\\\n\u538b\u6291\u8fdc\u53e4\u6d41\u7a9c\u7684\u86ee\u8352\u6697\u53f7\n\u800c\u7ba1\u98ce\u7434\u952e\u9ad8\u50b2\u7684\u8bf4 \u90a3\u53ea\u662f\u5728\u5f92\u52b3\n\u50b2\u50b2\u50b2\u50b2\u50b2 \u6211\u7684\u4e50\u5668\u5728\u73af\u7ed5\n\u65f6\u4ee3\u65e0\u6cd5\u6dd8\u6c70\u6211\u9738\u6c14\u7684\n\u4f60\u65e0\u6cd5\u9884\u8a00 \u56e0\u70ba\u6211\u8d8a\u9669\u7fc5\u8d8a\u8277\n\u6ca1\u6709\u53e5\u70b9 \u8de8\u65f6\u4ee3\u8513\u5ef6\u7ffc\u671d\u5929\n\u6708\u4e0b\u6d6e\u96d5 \u9b54\u9b3c\u7684\u6d45\\\u7b11\n\u72fc\u8fce\u98ce\u568e \u8760\u7fd4\u4f3c\u9ed1\u6f6e\n\u7528\u5b64\u72ec\u53bb\u8c03\u5c0a\u4e25\u7684\u8272\u8c03\n\u6211\u8de8\u8d8a\u8fc7\u65f6\u4ee3 \u5982\u517d\u822c\u7684\u59ff\u6001\n\u7434\u58f0\u5524\u9192\u6c88\u7761\u7684\u8840\u8109\n\u4e0d\u9700\u8981\u88ab\u5d07\u62dc \u5982\u517d\u822c\u7684\u60b2\u54c0\n\u53ea\u70ba\u6c38\u6052\u7684\u4e50\u66f2\u5b58\u5728 \u9192\u8fc7\u6765\n\u6211\u4e0d\u9700\u8981\u88ab\u5d07\u62dc\n\u6211\u4e0d\u9700\u8981\u88ab\u5d07\u62dc\n\u6211\u8de8\u8d8a\u8fc7\u65f6\u4ee3 \u5982\u517d\u822c\u7684\u59ff\u6001\n\u7434\u58f0\u5524\u9192\u6c88\u7761\u7684\u8840\u8109\n\u4e0d\u9700\u8981\u88ab\u5d07\u62dc \u5982\u517d\u822c\u7684\u60b2\u54c0\n\u53ea\u70ba\u6c38\u6052\u7684\u4e50\u66f2\u5b58\u5728 \u9192\u8fc7\u6765\n\u4f5c\u8bcd\uff1a\u53e4\u5c0f\u529b/\u9ec4\u51cc\u5609 \n"
 }
]
```

接着我们稍微处理下数据集。包括全部改为小写，去除换行符，然后截去后面一段使得接下来的训练会快一点。

```{.python .input  n=2}
time_machine = time_machine.lower().replace('\n', '').replace('\r', '')
corpus_chars = time_machine[0:10000]
```

## 字符的数值表示

先把数据里面所有不同的字符拿出来做成一个字典：

```{.python .input  n=3}
idx_to_char = list(set(corpus_chars))
char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])

vocab_size = len(char_to_idx)

print('vocab size:', vocab_size)
print(char_to_idx)
```

```{.json .output n=3}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "vocab size: 1166\n{'\u62ef': 0, '\u5f03': 1, '\u627e': 2, '\u9b42': 3, '\u77f3': 4, '\u5c01': 5, '\u4e1b': 6, '\u7edc': 7, '\u6574': 8, '\u5427': 9, '\u526a': 10, '\u665a': 11, '\u547d': 12, '\u7387': 13, '\u96f6': 14, '\u5f90': 15, '\u6728': 16, '\u9664': 17, '\u5e38': 18, '\u7948': 19, '\u5e97': 20, '\u6258': 21, '\u5598': 22, '\u5c3d': 23, '\u5c82': 24, '\u770b': 25, '\u6052': 26, '\u5bf9': 27, '\u9041': 28, '\u4e07': 29, '\u4e4e': 30, '\u5916': 31, '\u8ddd': 32, '\u697c': 33, '\u8ba8': 34, '/': 35, '\u6750': 36, '\u9003': 37, '\u5f88': 38, '\u8457': 39, '\u6b47': 40, '\u592b': 41, '\u971c': 42, '\u52a0': 43, '\u51ac': 44, '\u9675': 45, '\u8857': 46, '\u5d0e': 47, '\u4e09': 48, '\u719f': 49, '\u4e16': 50, '\u725b': 51, '\u753b': 52, '\u95e8': 53, '\u90ce': 54, '\u65f6': 55, '\u9677': 56, '\u68ad': 57, '\u9e25': 58, '\u8a00': 59, '\u6cea': 60, '\u54df': 61, '\u72f0': 62, '\u7eb7': 63, '\u8fd8': 64, '\u7a0e': 65, '\u4ece': 66, '\u5267': 67, '\u984f': 68, '\u6284': 69, '\u7b8f': 70, '\u952e': 71, '\u60f3': 72, '\u7d27': 73, '\u9a73': 74, '\u53e0': 75, '\u9b54': 76, '\u9713': 77, '\u83b9': 78, '\u629b': 79, ' ': 80, '\u5b99': 81, '\u975e': 82, '\u5fd8': 83, '\u607c': 84, '\u6735': 85, '\u53cb': 86, '\u538c': 87, '\u66f4': 88, '\u751c': 89, '\u7b49': 90, '\u5e74': 91, '\u8bb8': 92, '\u8361': 93, '\u9020': 94, '\u786c': 95, '\u9e7f': 96, '\u5a49': 97, '\u914b': 98, '\u5351': 99, '\u6b7b': 100, '\u7075': 101, '\u4e0d': 102, '\u4e0e': 103, '\u8bc6': 104, '\u7f16': 105, '\u5c0f': 106, '\u9875': 107, '\u51cc': 108, '\u6dcb': 109, '\u961f': 110, '\u6d77': 111, '\u5e7d': 112, '\u8f66': 113, '\u6495': 114, '\u6291': 115, '\u5b64': 116, '\u5929': 117, '\u6b21': 118, '\u4e8b': 119, '\u8d64': 120, '\u5c0a': 121, '\u6775': 122, '\u5e55': 123, '\u6210': 124, '\u8bf7': 125, '\u8138': 126, '\u9005': 127, '\u95ea': 128, '\u5165': 129, '\u8231': 130, '\u5409': 131, '\u5bfc': 132, '\u6240': 133, '\u9082': 134, '\u7bb1': 135, '\u7ae5': 136, '\u526f': 137, '\u4e70': 138, '\u72e0': 139, '\u9b45': 140, '\u62c9': 141, '\u7f9e': 142, '\u5f1f': 143, '\u62cb': 144, '\u521b': 145, '\u63a9': 146, '\u53c8': 147, '\u8def': 148, '\u4fa0': 149, '\u5473': 150, '\u79bb': 151, '\u6447': 152, '\u5939': 153, '\u7269': 154, '\u529e': 155, '\u51b3': 156, '\u8bb0': 157, '\u6a71': 158, '\u5200': 159, '\u62fd': 160, '\u8721': 161, '\u8868': 162, '\u601d': 163, '\u5954': 164, '\u7434': 165, '\u9669': 166, '\u5ce1': 167, 'p': 168, '\u94a8': 169, '\u62c6': 170, '\u5fa9': 171, '\u62cd': 172, '\u53e3': 173, '\u5411': 174, '\u6674': 175, '\u594f': 176, '\u65e9': 177, '\u817f': 178, '\u6751': 179, '\u950b': 180, '\u6b8b': 181, '\u6454': 182, '\u7eea': 183, '\u4f11': 184, '\u63a8': 185, '\u9996': 186, '\u5b50': 187, '\u9006': 188, '\u5f27': 189, '\u96be': 190, '\u9ece': 191, '\u4f60': 192, '\u6015': 193, '\u56db': 194, '\u4f9d': 195, '\u9f41': 196, '\u59ff': 197, '\u5b63': 198, '\u7eed': 199, '\u5462': 200, '\u7559': 201, '\u75af': 202, '\u504f': 203, '\u6c34': 204, '\u6e21': 205, '\u5199': 206, '\u9910': 207, '\u60b2': 208, '\u7a7a': 209, '\u8725': 210, 'b': 211, '\u5566': 212, '\u95f4': 213, '\u6297': 214, '\u4f1a': 215, '\u8c8c': 216, '\u5e86': 217, '\u6dd8': 218, '\u72fc': 219, '\u8fc7': 220, '\u65e7': 221, '\u7d2f': 222, '\u901f': 223, '\u62a2': 224, '\u62ac': 225, '\u591a': 226, '\u82b1': 227, '\u8033': 228, '\u62fc': 229, '\u8fa8': 230, '\u65b0': 231, '\u8c26': 232, '\u79fb': 233, '\u503a': 234, '\u6781': 235, '\u5c18': 236, '\u957f': 237, '\u5634': 238, '\u6eaa': 239, '\u8c03': 240, '\u5c16': 241, '\u4e8c': 242, '\u63d0': 243, '\u9aa8': 244, '\u7740': 245, '\u8dd1': 246, '\u98df': 247, '\u51b7': 248, '\u60dc': 249, '\u59cb': 250, 'l': 251, 'j': 252, '\u723d': 253, '\u62bd': 254, '\u5e7b': 255, '\u62e2': 256, '\u53ed': 257, '\u9650': 258, '\u9759': 259, '\u5708': 260, '\u5c41': 261, '\u91cf': 262, '\u8352': 263, '\u7535': 264, '\u88d9': 265, '\u5f7f': 266, '\u5e02': 267, '\u72d0': 268, '\u6bb5': 269, '\u6d6a': 270, '\u82f1': 271, '\u559d': 272, '\u7977': 273, '\u6811': 274, '\u558a': 275, '\u5356': 276, '\u9cc5': 277, '\u6bcd': 278, '\u6c14': 279, '\u9e70': 280, '\u8036': 281, '?': 282, '\u6cfd': 283, '\u8bd5': 284, '\u5bc2': 285, '\u9012': 286, '\u6bdb': 287, ')': 288, 'a': 289, '\u4f55': 290, '\u6808': 291, '\u6069': 292, '\u91ce': 293, '\\\\': 294, '\u888b': 295, '\u8543': 296, '\u54ea': 297, '\u5019': 298, '\u96e8': 299, '\u64cd': 300, '\u96fe': 301, '\u78b0': 302, '\u832b': 303, '\u505a': 304, '\u8154': 305, '\u4eb2': 306, '\u4e3e': 307, '\u591f': 308, '\u6194': 309, '\u9ebb': 310, '\u52db': 311, '\u6ed1': 312, '\u7a9d': 313, '\u602a': 314, '\u7fd8': 315, '\u7626': 316, '\u5f71': 317, '\u8fb9': 318, '\u4e25': 319, '\u5f15': 320, '\u6295': 321, '\u8587': 322, '\u6b65': 323, '\u904d': 324, '\u9ed1': 325, '\u5207': 326, '\u5e94': 327, '\u70db': 328, '\u67d4': 329, '\u6b20': 330, '\u751f': 331, '\u7239': 332, '\u90ca': 333, '\u8ffd': 334, '\u63a5': 335, '\u81ea': 336, '\u5fd9': 337, '\u7b11': 338, '\u8513': 339, '\u715e': 340, '\u4e86': 341, '\u7897': 342, '\u6c42': 343, '\u73a9': 344, '\u6797': 345, '\u70e6': 346, '\u76fc': 347, '\u91cc': 348, '\u675c': 349, '\u4f3c': 350, '\u8111': 351, '\u67af': 352, '\u4e3d': 353, '\u5370': 354, '\u75db': 355, '\u83b1': 356, '\u5fc5': 357, '\u8003': 358, '\u7eb8': 359, '\u4e5f': 360, '\u5bb6': 361, '\u6bd4': 362, '\u6ce5': 363, '\u4f53': 364, '\u7537': 365, '\u8679': 366, '\u79d2': 367, '\u4e22': 368, '\u5341': 369, '\u4ecd': 370, '\u7cd6': 371, '\u51e0': 372, '\u54cd': 373, '\u6a2a': 374, '\u5f2f': 375, '\u8349': 376, '\u5507': 377, '\u699c': 378, '\u9519': 379, '\u81c0': 380, '\u65e0': 381, '\u624d': 382, '\u8d76': 383, '\u5988': 384, '\u5931': 385, '\u661f': 386, '\u6ab3': 387, '\u51c9': 388, '\u5f53': 389, '\u775b': 390, '\u7ffb': 391, '\u6d6e': 392, '\u8ba4': 393, '\u6765': 394, '\u539f': 395, '\u68a6': 396, '\u800c': 397, '\u76ee': 398, '\u72de': 399, '\u5f62': 400, '\u9547': 401, '\u9ab7': 402, '\u9009': 403, '\u54c8': 404, '\u8089': 405, '\u7ed5': 406, '\u793e': 407, '\u8bf4': 408, '\u8a3b': 409, '\u8001': 410, '\u89e3': 411, '\u5938': 412, '\u767d': 413, '\u7070': 414, '\u5212': 415, '\u8f6c': 416, '\u4e0b': 417, '\u7ed8': 418, '\u653b': 419, '\u5bfa': 420, '\u9633': 421, '\u90fd': 422, '\u8d8a': 423, '\u5dfe': 424, '\u6068': 425, '\u9152': 426, '\u4e48': 427, '\u4fbf': 428, '\u5feb': 429, '\u7ec8': 430, '\u88ab': 431, 'g': 432, '\u538b': 433, '\u534e': 434, '\u6c88': 435, '\u8fce': 436, '\u98d8': 437, '\u672a': 438, '\u788e': 439, '\u6027': 440, '\u5927': 441, '\u53e5': 442, '\u627f': 443, '\u4ee4': 444, '\u914d': 445, '\u62a4': 446, '\u9017': 447, '\u5237': 448, '\u5317': 449, '\u5594': 450, '\u5bde': 451, '\u4eea': 452, '\u5999': 453, '\u5435': 454, '\u76f8': 455, '\u5e9f': 456, '\u4ec0': 457, '\u5f3a': 458, '\u751a': 459, '\u86c7': 460, '\u5466': 461, '\u74f6': 462, '\u6670': 463, '\u9999': 464, '\u592a': 465, '\u5e2e': 466, '\u6f20': 467, '\u8eb2': 468, '\u5757': 469, '\u9ea6': 470, '\u5578': 471, '\u6cb3': 472, '\u8fdb': 473, '\u89c1': 474, '\u5371': 475, '\u53f6': 476, '\u89c9': 477, '\u7ed9': 478, '\u7ed3': 479, '\u5668': 480, '\u51fa': 481, '\u6837': 482, '\u843d': 483, '\u5b88': 484, '\u65bc': 485, '\u4eec': 486, '\u540e': 487, '\u79d1': 488, '\u6e38': 489, '\u4e13': 490, '\u76ef': 491, '\u8865': 492, '\u5f8b': 493, '\u672c': 494, '\u58bb': 495, '\u98de': 496, '\u6770': 497, '\u8d35': 498, '\u52b3': 499, '\u72ec': 500, '\u6570': 501, '\u8de8': 502, '\u8150': 503, '\u8bcd': 504, '\u5c4c': 505, '\u4e88': 506, '\u8bd7': 507, '\u8ff7': 508, '\u4eae': 509, '\u501f': 510, '\u88c5': 511, '\\u3000': 512, '\u7f18': 513, '\u8277': 514, '\u8ddf': 515, '\u7761': 516, '\u53ef': 517, '\u8d77': 518, '\u4f20': 519, '\u67f3': 520, '\u5b83': 521, '\u6296': 522, '\u5e03': 523, '\u5524': 524, '\u5c42': 525, '\u4f24': 526, '\u5e95': 527, '\u9ad8': 528, '\u5f20': 529, '\u6298': 530, '\u7ecf': 531, '\u7a7f': 532, '\u80fd': 533, '\u584c': 534, '\u9738': 535, '\u5305': 536, '\u5316': 537, '\u5ba2': 538, '\u7518': 539, '\u676f': 540, '\u51b0': 541, '\u6cca': 542, '\u95f9': 543, '\u4eba': 544, '\u5883': 545, '\u8fde': 546, '\u8be5': 547, '.': 548, '\u9000': 549, '\u5357': 550, '\u903c': 551, '\u95ed': 552, '\u4e45': 553, '\u6491': 554, '\u8272': 555, '\u540c': 556, '\u95f7': 557, '\u666f': 558, '\u8c22': 559, '\u70ed': 560, '\u544a': 561, '\u519b': 562, '\u624b': 563, '\u5f26': 564, '\u4e34': 565, '\u5ff5': 566, '\u5fc3': 567, '\u4ed4': 568, '\u7529': 569, '\u6280': 570, '\u793c': 571, '\u5077': 572, '\u996e': 573, '\u4e58': 574, '\u8c46': 575, '\u79c0': 576, '\u518c': 577, '\u634f': 578, '\u671f': 579, '\u647a': 580, '\u6ef4': 581, '\u8981': 582, '\u9636': 583, '\u8109': 584, '\u9ed8': 585, '\u5de7': 586, '\u6599': 587, ':': 588, '\u5b87': 589, '\u8bc5': 590, '\u964d': 591, '\u8fdc': 592, '\u4f1f': 593, '\u60eb': 594, '\u4e0a': 595, '\u6545': 596, 'f': 597, '\u610f': 598, '\u7591': 599, '\u6839': 600, '\u8537': 601, '\u540d': 602, '\u89d2': 603, '\u9752': 604, '\u7ec6': 605, '\u90a3': 606, '\u90ae': 607, '\u56f4': 608, '\u524d': 609, '\u5f85': 610, '\u65cb': 611, '\u70df': 612, '\u704c': 613, '\u620f': 614, '\u5609': 615, '\u5173': 616, '\u76d8': 617, '\u7b28': 618, '\u66f2': 619, '\u4fef': 620, '\u6613': 621, '\u534a': 622, '\u95fb': 623, '\u5ead': 624, '\u5deb': 625, '\u82cd': 626, '\u7480': 627, '\u4fca': 628, '\u7a97': 629, '\u8fd0': 630, 'c': 631, '\u86ee': 632, '\u4e3a': 633, '\u5a18': 634, '\u9884': 635, '\u585e': 636, '\u7267': 637, '\u4e61': 638, '\u74a8': 639, '\u5982': 640, '\u86cb': 641, '\u6e34': 642, '\u60b4': 643, '\u80af': 644, '\u7247': 645, '\u5149': 646, '\u7483': 647, '\u62e5': 648, '\u73ab': 649, '\u6218': 650, '\u6a21': 651, '\u64ad': 652, '\u548c': 653, '\u684c': 654, '\u8c37': 655, '\u679d': 656, '\u4ee5': 657, '\u5440': 658, '\u9881': 659, '\u6cbc': 660, '\u70ba': 661, '\u7470': 662, '\u574f': 663, '\u5730': 664, '\u72d7': 665, '\u884c': 666, '\u53e4': 667, '\u660e': 668, '\u4f4d': 669, '\u6c5f': 670, '\u602f': 671, '\u997f': 672, '\u7136': 673, '\u8131': 674, '\u5374': 675, '\u50cf': 676, '\u63cd': 677, '\u96d5': 678, '\u5385': 679, '\u98ce': 680, '\u65e5': 681, '\u6c38': 682, '\u5976': 683, 'u': 684, '\u5f77': 685, '\u7e41': 686, '\u5904': 687, '(': 688, '\u94b1': 689, '\u8774': 690, '\u7ca5': 691, '\u8c8d': 692, '\u7687': 693, '\u5468': 694, '\u56fd': 695, '\u53c2': 696, '\u5438': 697, '\u6f6e': 698, '\u5802': 699, '\u804b': 700, '\u521d': 701, '\u8f7d': 702, '\u955c': 703, '\u675f': 704, '\u5728': 705, '\u62ff': 706, '\u6f2b': 707, '\u6c99': 708, '\u5de2': 709, '\u5d87': 710, '\u64ce': 711, '\u6401': 712, '\u9053': 713, '\u4ee3': 714, '\u66ff': 715, '\u5171': 716, '\u5947': 717, '\u653e': 718, '\u9ebc': 719, '\u6ee1': 720, '\u6c89': 721, '\u5d07': 722, '\u533a': 723, '\u7ffc': 724, '\u5178': 725, '\u6f14': 726, '\u4e60': 727, '\u7231': 728, '\u6ca1': 729, '\u539a': 730, '\u59fb': 731, '\u6e3a': 732, '\u8c61': 733, '\u654c': 734, '\u5fc6': 735, '\u6000': 736, '\u7167': 737, '\u52a8': 738, '\u61c2': 739, '\u6e05': 740, '\u561b': 741, '\u4e18': 742, '\u6d3b': 743, '\u6cd5': 744, '\u5403': 745, '\u6325': 746, '\u533b': 747, '\u5236': 748, '\u6643': 749, '[': 750, '\u5f7b': 751, '\u9662': 752, '\u79c3': 753, '\u6001': 754, '\u80cc': 755, 't': 756, '\u6daf': 757, '\u9187': 758, '\u4f0a': 759, 'm': 760, '\u7528': 761, '\u70b9': 762, '\u53f2': 763, '\u5f92': 764, '\u6bef': 765, '\u6bc5': 766, '\u6446': 767, '\u5821': 768, '\u6389': 769, '\u8222': 770, '\u4e24': 771, '\u559c': 772, '\u671d': 773, '\u822c': 774, '\u9886': 775, '\u6628': 776, '\u9762': 777, '\u5230': 778, '\u7ebf': 779, '\u9879': 780, '\u7981': 781, '\u4e50': 782, '\u732b': 783, '\u9b3c': 784, '\u8f97': 785, '\u65af': 786, '\u4e66': 787, '\u7fc5': 788, '\u6761': 789, '\u4f26': 790, 'o': 791, '\u7ea2': 792, '\u65ad': 793, '\u89c6': 794, '\u8f91': 795, '\u9057': 796, '\u5806': 797, '\u95ee': 798, '\u5496': 799, '\u6eb6': 800, '\u906e': 801, '\u8fc8': 802, '\u878d': 803, '\u6ce3': 804, '\u754c': 805, '\u671b': 806, '\u706b': 807, '\u94c1': 808, '\u8df3': 809, '\u8e1e': 810, '\u84dd': 811, '\u773c': 812, '\u67aa': 813, '\u4e4b': 814, '\u56fe': 815, '\u79cb': 816, '\u6bcf': 817, '\u5c3e': 818, '\u59a5': 819, '\u5b58': 820, '\u6e29': 821, '\u54a9': 822, '\u6cb9': 823, '\u59d0': 824, '\u5176': 825, '\u5174': 826, '\u695a': 827, '\u8c0e': 828, '\u4fe1': 829, '\u63a7': 830, '\u871c': 831, '\u6700': 832, '\u53d1': 833, '\u6b77': 834, '\u5b8c': 835, '\u5df1': 836, '\u73bb': 837, '\u8eba': 838, '\u7b14': 839, '\u5dee': 840, '\u9e20': 841, '\u8bed': 842, '\u7fa1': 843, '\u8840': 844, '\u65a4': 845, '\u628a': 846, '\u575a': 847, '\u968f': 848, '\u6c70': 849, '\u795d': 850, '\u8de1': 851, '\u6101': 852, '\u5145': 853, '\u6094': 854, '\u836f': 855, '\u4e2a': 856, '\u5f97': 857, '\u821e': 858, '\u7435': 859, '\u706f': 860, '\u76f4': 861, '\u60c5': 862, '\u6b63': 863, '\u8f7b': 864, '\u5355': 865, '\u8010': 866, '\u8861': 867, '\u7cd7': 868, '\u4ed9': 869, 'k': 870, 'i': 871, '\u6709': 872, '\u81f3': 873, '\u53bb': 874, 'e': 875, '\u5cb8': 876, '\u4e9b': 877, '\u7bee': 878, '\u611f': 879, '\u778e': 880, '\u771f': 881, '\u9605': 882, '\u8bdd': 883, '\u4f4f': 884, '\u60ef': 885, '\u793a': 886, '\u5915': 887, '\u6587': 888, '\u4e4c': 889, '\u5854': 890, '\u5de6': 891, '\u5530': 892, '\u77b0': 893, '\u6df1': 894, '\u9e26': 895, '\u7adf': 896, '\u5047': 897, '\u7406': 898, '\u4e00': 899, '\u4e49': 900, '\u638c': 901, '\u9010': 902, '\u7fd4': 903, '\u7531': 904, '\u542c': 905, '\u5934': 906, '\u7275': 907, '\u5148': 908, '\u670b': 909, '\u4f5c': 910, '\u91cd': 911, '\u97f3': 912, '\u52c9': 913, '\u679c': 914, '\u66b4': 915, '\u6c49': 916, '\u867d': 917, '\u6676': 918, '\u8fd1': 919, '\u673a': 920, '\u6c11': 921, '\u966a': 922, '\u8bbf': 923, '\u9a6c': 924, '\u5c1d': 925, '\u9700': 926, '\u94f6': 927, '\u6089': 928, '\u5b89': 929, '\u6536': 930, '\u6e56': 931, '\u7ba1': 932, '\u597d': 933, '\u683c': 934, '\u9192': 935, '\u73af': 936, '\u7b97': 937, '\u6708': 938, '\u5575': 939, '\u6597': 940, '\u9632': 941, '\u8d70': 942, '\u95f2': 943, '\u81fa': 944, '\u77ed': 945, '\u96ea': 946, '\u72c2': 947, '\u6012': 948, '\u5b81': 949, '\u8d25': 950, '\u5347': 951, '\u876a': 952, '\u6324': 953, '\u6211': 954, '\u58f0': 955, '\u5979': 956, '\u838e': 957, '\u5e72': 958, '\u730e': 959, '\u4efd': 960, '\u7f8e': 961, '\u9418': 962, '\u5f39': 963, '\u5582': 964, '\u568e': 965, '\u5ef6': 966, '\u76cf': 967, '\u5e26': 968, '\u6028': 969, '\u591c': 970, '\u7f8a': 971, '\u523a': 972, '\u77e5': 973, '\u5fe7': 974, '\u7b1b': 975, '\u5343': 976, '\u79cd': 977, '\u51a0': 978, '\u7444': 979, '\u503e': 980, '\u6361': 981, '\u8a8c': 982, '\u7684': 983, '\u8bc1': 984, '\u50b2': 985, '\u840e': 986, '\u65b9': 987, '\u6625': 988, '\u8bd1': 989, '\u75b2': 990, '\u600e': 991, '\u53f7': 992, '\u6d45': 993, '\u513f': 994, '\u5bb9': 995, '\u5269': 996, '\u6076': 997, '\u6b22': 998, '\u9ec4': 999, '\u987b': 1000, '\u5446': 1001, '\u613f': 1002, '\u8bb2': 1003, '\u6551': 1004, '\u6dc7': 1005, '\u554a': 1006, '\u8bf1': 1007, '\u522b': 1008, '\u8f6e': 1009, '\u4f3d': 1010, '\u5144': 1011, '\u5fcd': 1012, '\u573a': 1013, '\u8e48': 1014, '\u5c40': 1015, '\u6696': 1016, '\u5e73': 1017, '\u788c': 1018, '\u8d85': 1019, '\u8ba9': 1020, '\u795e': 1021, '\u5587': 1022, '\u70c1': 1023, '\u677f': 1024, '\u6559': 1025, '\u56e0': 1026, '\u8760': 1027, '\u529b': 1028, '\u641e': 1029, '\u8282': 1030, '\u7ad9': 1031, '\u58f6': 1032, '\u5df2': 1033, '\u5543': 1034, '\u6768': 1035, 'n': 1036, '\u8ba1': 1037, '\u8521': 1038, '\u5561': 1039, '\u5b66': 1040, '\u77ac': 1041, '\u6563': 1042, '\u662f': 1043, '\u56de': 1044, '\u53cd': 1045, '\u5ed3': 1046, '\u7403': 1047, '\u8bc9': 1048, '\u8d34': 1049, '\u900f': 1050, '\u4e91': 1051, '\u5766': 1052, '\u811a': 1053, '\u5410': 1054, '\u5f80': 1055, '\u517d': 1056, '\u5f04': 1057, '\u514d': 1058, '\u86e6': 1059, '\u75bc': 1060, '\u6307': 1061, '\u731c': 1062, '\u5168': 1063, '\u8f83': 1064, '\u5f55': 1065, '\u5e05': 1066, '\u6539': 1067, '\u4fb5': 1068, '\u82e5': 1069, '\u660f': 1070, '\u8776': 1071, '\u518d': 1072, '\u4ed6': 1073, '\u54a7': 1074, '\u5426': 1075, '\u813e': 1076, '\u6025': 1077, '\u5492': 1078, '\u9a82': 1079, '\u9b4f': 1080, '\u547c': 1081, '\uff1a': 1082, '\u54c1': 1083, '\u5ea7': 1084, '\u57ce': 1085, '\u53f3': 1086, '\u75f9': 1087, '\u56a3': 1088, '\u9760': 1089, '\u59b9': 1090, '\u5b9a': 1091, '\u67b6': 1092, '\u6253': 1093, '\u9891': 1094, '\u5973': 1095, '\u6309': 1096, '\u5b9e': 1097, '\u5ea6': 1098, '\u4f46': 1099, '\u6d41': 1100, '\u73cd': 1101, '\u6697': 1102, '\u5531': 1103, '\u8c01': 1104, '*': 1105, '\u82e6': 1106, '\u636e': 1107, '\u4f1e': 1108, '\u901a': 1109, '\u5206': 1110, '\u6311': 1111, '\u5012': 1112, '\u62ec': 1113, '\u7cfb': 1114, '\u6742': 1115, '\u54ce': 1116, '\u5f00': 1117, '\u5417': 1118, '\u5c71': 1119, '\u6eda': 1120, '\u73b0': 1121, '\u62b1': 1122, '\u9ac5': 1123, '\u7a9c': 1124, '\u6f02': 1125, '\u54c0': 1126, '\u53d8': 1127, 's': 1128, '\u505c': 1129, 'r': 1130, '\u6b4c': 1131, '\u4f73': 1132, '\u4e3b': 1133, '\u5fae': 1134, '\u53ea': 1135, '\u5348': 1136, '\u9898': 1137, '\u7436': 1138, 'h': 1139, '\u54ed': 1140, '\u6e10': 1141, '\u4e2d': 1142, '\u6162': 1143, '\u4f7f': 1144, '\u5956': 1145, '\u5bfb': 1146, '\u53eb': 1147, '\u72b6': 1148, '\u5e08': 1149, '\u74dc': 1150, '\u5c06': 1151, '\u65c1': 1152, '\u5bc4': 1153, ']': 1154, '\u987a': 1155, '\u5c31': 1156, '\u8bfa': 1157, '\u9897': 1158, '\u6d1b': 1159, '\u62dc': 1160, '\u8fd9': 1161, '\u5df7': 1162, '\u8eab': 1163, '\u6591': 1164, '\u5606': 1165}\n"
 }
]
```

然后可以把每个字符转成从0开始的指数(index)来方便之后的使用。

```{.python .input  n=4}
corpus_indices = [char_to_idx[char] for char in corpus_chars]

sample = corpus_indices[:40]

print('chars: \n', ''.join([idx_to_char[idx] for idx in sample]))
print('\nindices: \n', sample)
```

```{.json .output n=4}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "chars: \n \u4f5c\u8bcd\uff1a\u9ec4\u4fca\u90ce \u4f5c\u66f2\uff1a\u5468\u6770\u4f26\u7f16\u66f2\uff1a\u9ec4\u96e8\u52db\u9418\u9006\u65f6\u9418\u800c\u7ed5 \u6076\u7269\u72f0\u72de\u7684\u503e\u5de2\u6211\u8c26\u5351\u5b89\u9759\u7684\u65bc\n\nindices: \n [910, 504, 1082, 999, 628, 54, 80, 910, 619, 1082, 694, 497, 790, 105, 619, 1082, 999, 299, 311, 962, 188, 55, 962, 397, 406, 80, 997, 154, 62, 399, 983, 980, 709, 954, 232, 99, 929, 259, 983, 485]\n"
 }
]
```

## 数据读取

同前一样我们需要每次随机读取一些（`batch_size`个）样本和其对用的标号。这里的样本跟前面有点不一样，这里一个样本通常包含一系列连续的字符（前馈神经网络里可能每个字符作为一个样本）。

如果我们把序列长度（`seq_len`）设成10，那么一个可能的样本是`The Time T`。其对应的标号仍然是长为10的序列，每个字符是对应的样本里字符的后面那个。例如前面样本的标号就是`he Time Tr`。

下面代码每次从数据里随机采样一个批量：

```{.python .input  n=5}
import random
from mxnet import nd

def data_iter(batch_size, seq_len, ctx=None):
    num_examples = (len(corpus_indices)-1) // seq_len
    num_batches = num_examples // batch_size
    # 随机化样本
    idx = list(range(num_examples))
    random.shuffle(idx)
    # 返回seq_len个数据
    def _data(pos):
        return corpus_indices[pos:pos+seq_len]
    for i in range(num_batches):
        # 每次读取batch_size个随机样本
        i = i * batch_size
        examples = idx[i:i+batch_size]
        data = nd.array(
            [_data(j*seq_len) for j in examples], ctx=ctx)
        label = nd.array(
            [_data(j*seq_len+1) for j in examples], ctx=ctx)
        yield data, label
```

看下读出来长什么样：

```{.python .input  n=6}
for data, label in data_iter(batch_size=3, seq_len=8):
    print('data: ', data, '\n\nlabel:', label)
    break
```

```{.json .output n=6}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "data:  \n[[  709.   954.   232.    99.   929.   259.   983.   485.]\n [  883.    64.   102.  1043.    72.   343.   343.   192.]\n [  408.  1042.    80.   192.    72.    38.   553.   341.]]\n<NDArray 3x8 @cpu(0)> \n\nlabel: \n[[  954.   232.    99.   929.   259.   983.   485.  1085.]\n [   64.   102.  1043.    72.   343.   343.   192.  1044.]\n [ 1042.    80.   192.    72.    38.   553.   341.     9.]]\n<NDArray 3x8 @cpu(0)>\n"
 }
]
```

## 循环神经网络

在对输入输出数据有了解后，我们来正式介绍循环神经网络。

首先回忆下单隐层的前馈神经网络的定义，假设隐层的激活函数是$\phi$，那么这个隐层的输出就是

$$H = \phi(X W_{wh} + b_h)$$

最终的输出是

$$\hat{Y} = \text{softmax}(H W_{hy} + b_y)$$

（跟[多层感知机](../chapter_multilayer-neural-network/mlp-scratch.md)相比，这里我们把下标从$W_1$和$W_2$改成了意义更加明确的$W_{wh}$和$W_{hy}$)

将上面网络改成循环神经网络，我们首先对输入输出加上时间戳$t$。假设$X_t$是序列中的第$t$个输入，对应的隐层输出和最终输出是$H_t$和$\hat{Y}_t$。循环神经网络只需要在计算隐层的输出的时候加上跟前一时间输入的加权和，为此我们引入一个新的可学习的权重$W_{hh}$：

$$H_t = \phi(X_t  W_{xh} + H_{t-1} W_{hh} + b_h )$$

输出的计算跟前一致：

$$\hat{Y}_t = \text{softmax}(H_t W_{hy} + b_y)$$

一开始我们提到过，隐层输出（又叫隐藏状态）可以认为是这个网络的记忆。它存储前面时间里面的信息。我们的输出是完全只基于这个状态。最开始的状态，$H_{-1}$，通常会被初始为0.

## Onehot编码

注意到每个字符现在是用一个整数来表示，而输入进网络我们需要一个定长的向量。一个常用的办法是使用onehot来将其表示成向量。就是说，如果值是$i$, 那么我们创建一个全0的长为`vocab_size`的向量，并将其第$i$位表示成1.

```{.python .input  n=7}
nd.one_hot(nd.array([0,4]), vocab_size)
```

```{.json .output n=7}
[
 {
  "data": {
   "text/plain": "\n[[ 1.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n<NDArray 2x1166 @cpu(0)>"
  },
  "execution_count": 7,
  "metadata": {},
  "output_type": "execute_result"
 }
]
```

记得前面我们每次得到的数据是一个`batch_size x seq_len`的批量。下面这个函数将其转换成`seq_len`个可以输入进网络的`batch_size x vocba_size`的矩阵

```{.python .input  n=8}
def get_inputs(data):
    return [nd.one_hot(X, vocab_size) for X in data.T]

inputs = get_inputs(data)
print('input length: ',len(inputs))
print('input[0] shape: ', inputs[0].shape)
```

```{.json .output n=8}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "input length:  8\ninput[0] shape:  (3, 1166)\n"
 }
]
```

## 初始化模型参数

模型的输入和输出维度都是`vocab_size`。

```{.python .input  n=9}
import mxnet as mx

# 尝试使用 GPU
import sys
sys.path.append('..')
import utils
ctx = utils.try_gpu()
print('Will use ', ctx)

num_hidden = 256
weight_scale = .01

# 隐含层
Wxh = nd.random_normal(shape=(vocab_size,num_hidden), ctx=ctx) * weight_scale
Whh = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx) * weight_scale
bh = nd.zeros(num_hidden, ctx=ctx)
# 输出层
Why = nd.random_normal(shape=(num_hidden,vocab_size), ctx=ctx) * weight_scale
by = nd.zeros(vocab_size, ctx=ctx)

params = [Wxh, Whh, bh, Why, by]
for param in params:
    param.attach_grad()
```

```{.json .output n=9}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Will use  cpu(0)\n"
 }
]
```

## 定义模型

我们将前面的模型公式定义直接写成代码。

```{.python .input  n=10}
def rnn(inputs, H):
    # inputs: seq_len 个 batch_size x vocab_size 矩阵
    # H: batch_size x num_hidden 矩阵
    # outputs: seq_len 个 batch_size x vocab_size 矩阵
    outputs = []
    for X in inputs:
        H = nd.tanh(nd.dot(X, Wxh) + nd.dot(H, Whh) + bh)
        Y = nd.dot(H, Why) + by
        outputs.append(Y)
    return (outputs, H)
```

做个简单的测试：

```{.python .input  n=11}
state = nd.zeros(shape=(data.shape[0], num_hidden), ctx=ctx)
outputs, state_new = rnn(get_inputs(data.as_in_context(ctx)), state)

print('output length: ',len(outputs))
print('output[0] shape: ', outputs[0].shape)
print('state shape: ', state_new.shape)
```

```{.json .output n=11}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "output length:  8\noutput[0] shape:  (3, 1166)\nstate shape:  (3, 256)\n"
 }
]
```

## 预测序列

在做预测时我们只需要给定时间0的输入和起始隐藏状态。然后我们每次将上一个时间的输出作为下一个时间的输入。

![](../img/rnn_3.png)

```{.python .input  n=12}
def predict(prefix, num_chars):
    # 预测以 prefix 开始的接下来的 num_chars 个字符
    prefix = prefix.lower()
    state = nd.zeros(shape=(1, num_hidden), ctx=ctx)
    output = [char_to_idx[prefix[0]]]
    for i in range(num_chars+len(prefix)):
        X = nd.array([output[-1]], ctx=ctx)
        Y, state = rnn(get_inputs(X), state)
        #print(Y)
        if i < len(prefix)-1:
            next_input = char_to_idx[prefix[i+1]]
        else:
            next_input = int(Y[0].argmax(axis=1).asscalar())
        output.append(next_input)
    return ''.join([idx_to_char[i] for i in output])
```

## 梯度剪裁

在求梯度时，循环神经网络因为需要反复做`O(seq_len)`次乘法，有可能会有数值稳定性问题。（想想 $2^{40}$和$0.5^{40}$）。一个常用的做法是如果梯度特别大，那么就投影到一个比较小的尺度上。假设我们把所有梯度接成一个向量 $\boldsymbol{g}$，假设剪裁的阈值是$\theta$，那么我们这样剪裁使得$\|\boldsymbol{g}\|$不会超过$\theta$：

$$ \boldsymbol{g} = \min\left(\frac{\theta}{\|\boldsymbol{g}\|}, 1\right)\boldsymbol{g}$$

```{.python .input  n=13}
def grad_clipping(params, theta):
    norm = nd.array([0.0], ctx)
    for p in params:
        norm += nd.sum(p.grad ** 2)
    norm = nd.sqrt(norm).asscalar()
    if norm > theta:
        for p in params:
            p.grad[:] *= theta/norm
```

## 训练模型

下面我们可以还是训练模型。跟前面前置网络的教程比，这里只有两个不同。

1. 通常我们使用Perplexit(PPL)这个指标。可以简单的认为就是对交叉熵做exp运算使得数值更好读。
2. 在更新前我们对梯度做剪裁

```{.python .input  n=14}
#seq1 = 'The Time Ma'
#seq2 = "The Medical Man rose, came to the lamp,"
seq1 = '为什么'
seq2 = "为什么这样子"

from mxnet import autograd
from mxnet import gluon
from math import exp

epochs = 200
seq_len = 35
learning_rate = .1
batch_size = 32

softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()

for e in range(epochs+1):
    train_loss, num_examples = 0, 0
    state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)
    for data, label in data_iter(batch_size, seq_len, ctx):
        with autograd.record():
            outputs, state = rnn(get_inputs(data), state)
            # reshape label to (batch_size*seq_len, )
            # concate outputs to (batch_size*seq_len, vocab_size)
            label = label.T.reshape((-1,))
            outputs = nd.concat(*outputs, dim=0)
            loss = softmax_cross_entropy(outputs, label)
        loss.backward()

        grad_clipping(params, 5)
        utils.SGD(params, learning_rate)

        train_loss += nd.sum(loss).asscalar()
        num_examples += loss.size

    if e % 20 == 0:
        print("Epoch %d. PPL %f" % (e, exp(train_loss/num_examples)))
        print(' - ', predict(seq1, 100))
        print(' - ', predict(seq2, 100), '\n')

```

```{.json .output n=None}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Epoch 0. PPL 969.832335\n -  \u4e3a\u4ec0\u4e48                                                                                                     \n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50                                                                                                      \n\nEpoch 20. PPL 412.710435\n -  \u4e3a\u4ec0\u4e48\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211\u6211 \n\nEpoch 40. PPL 363.571047\n -  \u4e3a\u4ec0\u4e48\u7684 \u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50\u7684 \u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211 \n\nEpoch 60. PPL 254.937469\n -  \u4e3a\u4ec0\u4e48 \u4f60\u4e0d\u4e86\u4f60\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50 \u6211\u4e0d\u4e86\u4f60\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684\u53ef\u7231\u5973\u4eba\u7684 \n\n"
 }
]
```

可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。

## 结论

通过隐藏状态，循环神经网络很够更好的使用数据里的时序信息。

## 练习

调调参数（数据集大小，模型复杂度，学习率），看看对Perplexity和预测的结果造成的区别。

**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/989)
