# 循环神经网络 --- 从0开始

前面的教程里我们使用的网络都属于**前馈神经网络**。之所以叫前馈，是因为整个网络是一条链（回想下`gluon.nn.Sequential`），每一层的结果都是反馈给下一层。这一节我们介绍**循环神经网络**，这里每一层不仅输出给下一层，同时还输出一个**隐藏状态**，给当前层在处理下一个样本时使用。下图展示这两种网络的区别。

![](../img/rnn_1.png)

循环神经网络的这种结构使得它适合处理前后有依赖关系的样本。我们拿语言模型举个例子来解释这个是怎么工作的。语言模型的任务是给定句子的前*T*个字符，然后预测第*T+1*个字符。假设我们的句子是“你好世界”，使用前馈神经网络来预测的一个做法是，在时间1输入“你”，预测”好“，时间2向同一个网络输入“好”预测“世”。下图左边展示了这个过程。

![](../img/rnn_2.png)

注意到一个问题是，当我们预测“世”的时候只给了“好”这个输入，而完全忽略了“你”。直觉上“你”这个词应该对这次的预测比较重要。虽然这个问题通常可以通过**n-gram**来缓解，就是说预测第*T+1*个字符的时候，我们输入前*n*个字符。如果*n=1*，那就是我们这里用的。我们可以增大*n*来使得输入含有更多信息。但我们不能任意增大*n*，因为这样通常带来模型复杂度的增加从而导致需要大量数据和计算来训练模型。

循环神经网络使用一个隐藏状态来记录前面看到的数据来帮助当前预测。上图右边展示了这个过程。在预测“好”的时候，我们输出一个隐藏状态。我们用这个状态和新的输入“好”来一起预测“世”，然后同时输出一个更新过的隐藏状态。我们希望前面的信息能够保存在这个隐藏状态里，从而提升预测效果。

在更加正式的介绍这个模型前，我们先去弄一个比“你好世界“稍微复杂点的数据。


## 周杰伦歌词数据集

这里我们使用周杰伦歌词数据集。该数据集里包含了著名创作型歌手周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》所有歌曲的歌词。

![](../img/jay.jpg)



下面我们读取这个数据并看看前面49个字符（char）是什么样的：

```{.python .input  n=24}
import zipfile
with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip', 'r') as zin:
    zin.extractall('../data/')

with open('../data/jaychou_lyrics.txt') as f:
    corpus_chars = f.read()
print(corpus_chars[0:49])
```

```{.json .output n=24}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "\u60f3\u8981\u6709\u76f4\u5347\u673a\n\u60f3\u8981\u548c\u4f60\u98de\u5230\u5b87\u5b99\u53bb\n\u60f3\u8981\u548c\u4f60\u878d\u5316\u5728\u4e00\u8d77\n\u878d\u5316\u5728\u5b87\u5b99\u91cc\n\u6211\u6bcf\u5929\u6bcf\u5929\u6bcf\u5929\u5728\u60f3\u60f3\u60f3\u60f3\u8457\u4f60\n\n"
 }
]
```

接着我们稍微处理下数据集。为了打印方便，我们把换行符替换成空格，然后截去后面一段使得接下来的训练会快一点。

```{.python .input  n=2}
corpus_chars = corpus_chars.replace('\n', ' ').replace('\r', ' ')
corpus_chars = corpus_chars[0:10000]
```

## 字符的数值表示

先把数据里面所有不同的字符拿出来做成一个字典：

```{.python .input  n=3}
idx_to_char = list(set(corpus_chars))
char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])

vocab_size = len(char_to_idx)

print('vocab size:', vocab_size)
print(char_to_idx)
```

```{.json .output n=3}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "vocab size: 1160\n{'\u5728': 0, '\u5305': 1, '\u6dc7': 2, '(': 3, '\u6194': 4, '\u62e2': 5, '\u7ecf': 6, '\u59ff': 7, '\u5c82': 8, '\u5915': 9, '\u5feb': 10, '\u9910': 11, '\u7ca5': 12, '\u513f': 13, '\u9082': 14, '\u70ed': 15, '\u62fd': 16, '\u5e08': 17, '\u5fc5': 18, '\u96be': 19, '\u6e38': 20, '\u9875': 21, '\u79c0': 22, '\u63cd': 23, '\u60eb': 24, '\u9ed8': 25, '\u86cb': 26, '~': 27, '\u771f': 28, '\u6c49': 29, '\u6cca': 30, '\u98ce': 31, 'a': 32, '\u5212': 33, '\u8eba': 34, '\u5f00': 35, '\u666f': 36, '\u4f11': 37, '\u5f03': 38, '\u8c26': 39, '\u6d6a': 40, '\u7761': 41, '\u5f71': 42, '\u6700': 43, 'h': 44, '\u7eed': 45, '\u6e3a': 46, '\u964d': 47, '\u5377': 48, '\u8d77': 49, '\u4ece': 50, '\u751f': 51, '\u838e': 52, '\u518c': 53, '\u70b9': 54, '\u7518': 55, '\u8be5': 56, '\u54ce': 57, '\u7ae5': 58, '\u6309': 59, '\u6291': 60, '\u5e38': 61, '\u795d': 62, '\u5374': 63, '\u725b': 64, '\u5c3e': 65, '\u53d8': 66, '\u7a0e': 67, '\u6025': 68, '\u5e94': 69, '\u8de8': 70, '\u793e': 71, '\u80cc': 72, '\u4e4c': 73, '\u559c': 74, '\u5f15': 75, '\u6f2b': 76, '\u8d34': 77, '\u5ed3': 78, '\u5cb8': 79, '\u800c': 80, '\u5200': 81, '\u6015': 82, '\u8fa8': 83, '\u6d3b': 84, '\u5b99': 85, '\u54cd': 86, '\u8f6e': 87, '\u53e0': 88, '\u6258': 89, '\u76d8': 90, '\u5f7b': 91, '\u5e2e': 92, '\u7387': 93, '\u8089': 94, '\u522b': 95, '\u9519': 96, '\u6551': 97, '\u9b4f': 98, '\u7531': 99, '\u94b1': 100, '\u653b': 101, '\u6162': 102, '\u5c0f': 103, '\u6696': 104, '\u75f9': 105, '\u7528': 106, '\u9884': 107, '\u8865': 108, '\u63a5': 109, '\u526a': 110, '\u9b42': 111, '\u62dc': 112, '\u65c1': 113, '\u6bd4': 114, '\u653e': 115, '\u7cfb': 116, '\u5575': 117, '\u8513': 118, '\u94f6': 119, '\u5473': 120, '\u81f3': 121, '\u5e7d': 122, '\u53e4': 123, '\u4ee4': 124, '\u5e72': 125, '\u5f20': 126, '\u8d64': 127, '\u697c': 128, '\u4f5c': 129, '\u6cbc': 130, '\u7bee': 131, '\u968f': 132, '\u62fc': 133, '\u5904': 134, '\u6a71': 135, '\u60dc': 136, '\u5982': 137, '\u6495': 138, '\u7167': 139, '\u5fcd': 140, '\u5c4c': 141, '\u561b': 142, '\u4e25': 143, '\u8231': 144, '\u6bcd': 145, '\u8d35': 146, '\u53f6': 147, '\u4e9b': 148, '\u4ed6': 149, '\u59b9': 150, '\u8fc7': 151, '\u6670': 152, '\u5de2': 153, '\u5f7f': 154, '\u4ecd': 155, '\u4e66': 156, '\u7977': 157, '\u6674': 158, '\u641e': 159, '\u84dd': 160, '\u538b': 161, '\u6f14': 162, '\u9898': 163, '\u83b9': 164, '\u5561': 165, '\u7fc5': 166, '\u6d41': 167, '\u6101': 168, '\u521b': 169, '\u5316': 170, '\u6708': 171, '\u4f24': 172, '\u6068': 173, '\u4e2d': 174, '\u5999': 175, '\u5348': 176, '\u78b0': 177, '\u7981': 178, '\u5939': 179, '\u5c01': 180, '\u4ed4': 181, '\u526f': 182, '\u5149': 183, '\u821e': 184, '\u7fd8': 185, '\u8bc5': 186, '\u9e26': 187, '\u53e3': 188, '\u96d5': 189, '\u73ab': 190, '\u611f': 191, '\u7626': 192, '\u671f': 193, '\u6597': 194, '\u81fa': 195, '[': 196, '\u660f': 197, '\u68a6': 198, '\u671b': 199, '\u5199': 200, '\u8ba1': 201, '\u8857': 202, '\u903c': 203, '\u975e': 204, '\u8bd7': 205, '\u72de': 206, '\u4e0e': 207, '\u60c5': 208, '\u53f2': 209, '\u5bb6': 210, '\u8def': 211, '\u6e05': 212, '\u8f83': 213, '\u76ef': 214, '\u76ee': 215, '\u661f': 216, '\u4e34': 217, '\u54c8': 218, '\u5356': 219, '\u62c6': 220, '\u6770': 221, '\u6cb9': 222, '\u8c61': 223, '\u57ce': 224, '\u8df3': 225, '\u7ed3': 226, '\u8e48': 227, '\u60b4': 228, '\u4e0a': 229, '\u5351': 230, '\u5fc3': 231, '\u901f': 232, '\u73cd': 233, '\u5854': 234, '\u51ac': 235, '\u4f9d': 236, '\u4f4f': 237, '\u6591': 238, '\u7f18': 239, '\u9752': 240, '\u73a9': 241, '\u539a': 242, '\u5438': 243, '\u51fa': 244, '\u4f0a': 245, '\u5403': 246, '\u5317': 247, '\u7cd6': 248, '\u793a': 249, '\u7948': 250, '\u775b': 251, '\u88d9': 252, '\\\\': 253, '\u524d': 254, '\u5fd8': 255, '\u79bb': 256, '\u8109': 257, '\u94a8': 258, '\u8bc1': 259, '\u8f7d': 260, '\u503a': 261, '\u4e48': 262, '\u65af': 263, '\u5e05': 264, '\u54c0': 265, '\u5bb9': 266, '\u6c5f': 267, '\u8c8d': 268, '\u8c01': 269, '\u6781': 270, '\u4e60': 271, '\u7a7f': 272, '\u585e': 273, '\u8725': 274, '\u5708': 275, '\u9ebc': 276, '\u7a9d': 277, '\u9ec4': 278, '\u53ef': 279, '\u8282': 280, '\u6628': 281, '\u987b': 282, '\u75bc': 283, '\u4f46': 284, '\u5deb': 285, '\u8eab': 286, '\u5171': 287, '\u4e24': 288, '\u8fdb': 289, '\u5ce1': 290, '\u610f': 291, '\u65e0': 292, '\u75af': 293, '\u89c9': 294, '\u5531': 295, '\u6c14': 296, '\u7b14': 297, '\u8bf1': 298, '\u5f55': 299, '\u4eba': 300, '\u5b9a': 301, '\u8bb8': 302, '\u96e8': 303, '\u6761': 304, '\u5634': 305, '\u6bcf': 306, '\u5435': 307, 'b': 308, '\u594f': 309, '\u5931': 310, '\u6545': 311, '\u4fb5': 312, '\u6f6e': 313, '\u5976': 314, '\u7269': 315, '\u5668': 316, '\u7ed9': 317, '\u5594': 318, '\u8bf7': 319, '\u795e': 320, '\u7b11': 321, '\u52a8': 322, '\u7b8f': 323, '\u9cc5': 324, '\u8150': 325, '\u6c38': 326, '\u66f2': 327, '\u8bc6': 328, '\u4e18': 329, '\u6089': 330, '\u6240': 331, '\u53eb': 332, '\u95ed': 333, '\u4f26': 334, '\u8774': 335, '\u997f': 336, '\u4e0d': 337, '\u4e16': 338, '\u97f3': 339, '\u6324': 340, '\u731c': 341, '\u7275': 342, '\u6574': 343, '\u7480': 344, '\u8111': 345, '\u534e': 346, '\u4e2a': 347, '\u6401': 348, '\u67d4': 349, '\u4e50': 350, '\u65f6': 351, '\u77ac': 352, '*': 353, '\u75db': 354, '\u8bc9': 355, '\u53ca': 356, '\u867d': 357, '\u6b22': 358, '\u662f': 359, '\u89c6': 360, '.': 361, '\u67af': 362, '\u7436': 363, '\u5e86': 364, '\u67f3': 365, '\u6284': 366, '\u5b88': 367, '\u574f': 368, '\u8fde': 369, '\u6daf': 370, '\u89d2': 371, '\u8bed': 372, '\u6b20': 373, '\u63a7': 374, '\u5417': 375, '\u9a6c': 376, '\u8721': 377, '\u86e6': 378, '\u8d85': 379, '\u7075': 380, '\u9b54': 381, '\u5145': 382, '\u984f': 383, '\u5439': 384, '\u6ee1': 385, '\u5ba2': 386, '\u59a5': 387, '\u9e25': 388, '\u6742': 389, '\u60ef': 390, '\u5d07': 391, '\u6676': 392, '\u6b47': 393, '\u832b': 394, '\u53f7': 395, '\u6728': 396, '\u8277': 397, '\u5f88': 398, '\u5c16': 399, '\u8ffd': 400, '\u5f2f': 401, '\u7136': 402, '\u544a': 403, '\u4f3c': 404, '\u9a82': 405, '\u7ffc': 406, '\u573a': 407, '\u8ddf': 408, '\u5bfb': 409, '\u7687': 410, 'o': 411, '\u6613': 412, '\u62bd': 413, '\u4eec': 414, '\u548c': 415, '\u5947': 416, '\u5267': 417, '\u591a': 418, '\u60f3': 419, '\u9669': 420, '\u5012': 421, '\u65b9': 422, '\u62ac': 423, '\u9006': 424, '\u540c': 425, '\u7d27': 426, '\u54a9': 427, '\u95fb': 428, '\u60b2': 429, '\u9aa8': 430, '\u778e': 431, '\u542c': 432, '\u50cf': 433, '\u9e20': 434, '\u538c': 435, '\u91cc': 436, '\u8868': 437, '\u6625': 438, '\u5e9f': 439, '\u521d': 440, '\u753b': 441, '\u73b0': 442, '\u5530': 443, '\u6069': 444, '\u592b': 445, '\u7a9c': 446, '\u82e6': 447, '\u9633': 448, '\u955c': 449, '\u6491': 450, '\u5357': 451, '\u72c2': 452, '\u6211': 453, '\u79d2': 454, '\u65e5': 455, '\u6b63': 456, '\u5c41': 457, '\u6837': 458, '\u5173': 459, '\u98d8': 460, '\u72e0': 461, '\u82e5': 462, '\u660e': 463, 'n': 464, '\u5e97': 465, '\u6ed1': 466, '\u8138': 467, '\u8d8a': 468, '\u8033': 469, '\u547d': 470, '\u70c1': 471, '\u8c22': 472, '\u5bde': 473, '\u82cd': 474, '\u7529': 475, '\u6297': 476, '\u811a': 477, '\u66f4': 478, '\u7bb1': 479, '\u6eaa': 480, '\u568e': 481, '\u8537': 482, '\u74a8': 483, '\u8de1': 484, '\u540e': 485, '\u6094': 486, '\u6218': 487, '\u843d': 488, '\u5df1': 489, '\u5ea7': 490, '\u53cd': 491, '\u732b': 492, 'p': 493, '\u6cd5': 494, '\u751a': 495, '\u5f77': 496, '\u5934': 497, '\u7cd7': 498, '\u70e6': 499, '\u5fc6': 500, '\u6076': 501, '\u5c18': 502, '\u5fd9': 503, '\u6735': 504, '\u5178': 505, '\u5b89': 506, '\u4f7f': 507, '\u8f97': 508, '\u4f53': 509, '\u8fd8': 510, '\u91cd': 511, '\u607c': 512, '\u4ee5': 513, '\u5c31': 514, '\u5fe7': 515, '\u767d': 516, '\u5b9e': 517, '\u6709': 518, '\u5802': 519, '\u8ff7': 520, '\u9e70': 521, '\u529b': 522, '\u5b83': 523, '\u79fb': 524, '\u7d2f': 525, '\u62c9': 526, '\u6bc5': 527, '\u676f': 528, '\u76f4': 529, '\u6ef4': 530, '\u706b': 531, '\u901a': 532, '\u91ce': 533, '\u8d76': 534, ')': 535, '\u5440': 536, '\u64ad': 537, '\u7406': 538, '\u533b': 539, '\u8fb9': 540, '\u5e55': 541, '\u63a8': 542, '\u9003': 543, '\u8a3b': 544, '\u8f66': 545, '\u5236': 546, '\u5446': 547, '\u56de': 548, '\u9005': 549, '\u9ece': 550, '\u54c1': 551, '\u7267': 552, '\u6587': 553, '\u9760': 554, '\u773c': 555, '\u4ed9': 556, '\u53d7': 557, '\u5df2': 558, '\u7483': 559, '\u8587': 560, '\u7adf': 561, '\u8f7b': 562, '\u8f6c': 563, '\u5206': 564, '\u5f26': 565, '\u871c': 566, '\u8c03': 567, '\u6d1b': 568, '\u966a': 569, '\u6325': 570, '\u8e1e': 571, '\u6b7b': 572, '\u9650': 573, '\u9b3c': 574, '\u9897': 575, '\u72ec': 576, '\u8bd5': 577, '\u5587': 578, '\u6446': 579, '\u5f97': 580, '\u59d0': 581, '\u679d': 582, '\u5c1d': 583, '\u6c99': 584, '\u575a': 585, '\u6e34': 586, '\u4e0b': 587, '\u7eb8': 588, '\u961f': 589, '\u9713': 590, '\u547c': 591, '\u4f1e': 592, '\u96ea': 593, '\u65bc': 594, '\u6ca1': 595, '\u5582': 596, '\u5f3a': 597, '\u8776': 598, '\u5948': 599, '\u9418': 600, '\u723d': 601, '\u96f6': 602, '\u822c': 603, '\u6dcb': 604, '\u66ff': 605, '\u8c46': 606, '\u9e7f': 607, '\u7b97': 608, '\u7559': 609, '\u95f9': 610, '\u6052': 611, '\u518d': 612, '\u5a49': 613, '\u65cb': 614, '\u5d87': 615, '\u4fef': 616, '\u9009': 617, '\u5de6': 618, 't': 619, '\u634f': 620, '\u601d': 621, '\u5de7': 622, '\u5929': 623, '\u5047': 624, '\u628a': 625, '\u9881': 626, '\u62ef': 627, '\u81c0': 628, '\u68ad': 629, '\u6253': 630, '\u9b45': 631, '\u5598': 632, '\u9010': 633, '\u505c': 634, '\u6000': 635, '\u79c3': 636, '\u8c0e': 637, '\u9017': 638, '\u6389': 639, '\u7684': 640, '\u9ea6': 641, '\u906e': 642, '\u4e1b': 643, '\u72d7': 644, '\u7403': 645, '\u6808': 646, '\u4f20': 647, '\u7eb7': 648, '\u9000': 649, '\u4e3e': 650, '\u4e45': 651, '\u5b63': 652, '\u6f02': 653, '\u8bb0': 654, '\u9891': 655, '\u8010': 656, 'y': 657, '\u6dd8': 658, '\u534a': 659, '\u8861': 660, '\u6d6e': 661, '\u9662': 662, '\u7ebf': 663, '\u7b28': 664, '\u9152': 665, '\u6e29': 666, '\u6eb6': 667, '\u4f73': 668, '\u6d45': 669, '\u6454': 670, '\u5954': 671, '\u6df1': 672, '\u58bb': 673, '\u9020': 674, '\u76cf': 675, '\u6c70': 676, '\u6536': 677, '\u7ad9': 678, '\u4f1f': 679, '\u840e': 680, '\u9999': 681, '\u5fa9': 682, '\u8fd1': 683, '\u77b0': 684, '\u74dc': 685, '\u6539': 686, '\u4efd': 687, '\u8bd1': 688, '\u9ebb': 689, '\u4eea': 690, '\u884c': 691, '\u82b1': 692, '\u8154': 693, '\u62ff': 694, '\u704c': 695, '\u5492': 696, '\u95ea': 697, '\u4e07': 698, '\u80fd': 699, '\u5207': 700, '\u591f': 701, '\u8bbf': 702, '\u5c0a': 703, '\u53d1': 704, '\u558a': 705, '\u4e00': 706, '\u8760': 707, '\u64ce': 708, '\u65e9': 709, '\u878d': 710, '\u7ea2': 711, '\u6b4c': 712, '\u950b': 713, '\u6e21': 714, '\u59cb': 715, '\u8f91': 716, '\u8003': 717, '\u4e88': 718, '\u8bfa': 719, '\u519b': 720, '\u5757': 721, '\u7ec6': 722, '\u5fae': 723, '\u6e10': 724, '\u77f3': 725, '\u5956': 726, '\u62b1': 727, '\u8222': 728, '\u4ec0': 729, '\u9738': 730, '\u9996': 731, '\u95e8': 732, '\u7591': 733, '\u5148': 734, '\u6028': 735, '\u602a': 736, '\u602f': 737, '\u6c42': 738, '\u5e73': 739, '\u647a': 740, '\u5606': 741, '\u514d': 742, '\u4e8b': 743, '\u788c': 744, '\u8d70': 745, '\u7ed8': 746, '\u5927': 747, '\u754c': 748, '\u6a2a': 749, '\u51b0': 750, '\u70df': 751, '\u8ba8': 752, '\u4ee3': 753, '\u5bfa': 754, '\u5806': 755, '\u67b6': 756, '\u5b8c': 757, '\u9759': 758, '\u675f': 759, '\u5019': 760, '\u5c40': 761, '\u8036': 762, '\u6697': 763, '\u5f27': 764, '\u9547': 765, '\u638c': 766, '\u5355': 767, '\u7239': 768, '\u9636': 769, '\u817f': 770, '\u4e61': 771, '\u6361': 772, '\u4e8c': 773, '\u7e41': 774, '\u957f': 775, '\u6b8b': 776, '\u96fe': 777, '\u51a0': 778, '\u6c34': 779, '\u8349': 780, '\u8543': 781, '\u540d': 782, '\u88ab': 783, '\u523a': 784, '\u5730': 785, '\u5f92': 786, '\u6311': 787, '\u554a': 788, '\u671d': 789, '\u62d6': 790, '\u584c': 791, '\u5f53': 792, '\u77ed': 793, '\u58f6': 794, '\u9053': 795, '\u51e0': 796, '\u5c71': 797, '\u600e': 798, '\u715e': 799, 'm': 800, '\u8fce': 801, '\u76f8': 802, '\u673a': 803, '\u670b': 804, '\u9187': 805, '\u98df': 806, '\u64cd': 807, '\u53bb': 808, '\u7fa1': 809, '\u72d0': 810, '\u4e4e': 811, '\u5370': 812, '\u6b65': 813, '\u7edc': 814, '\u95f4': 815, '\u836f': 816, '\u7f9e': 817, '\u770b': 818, '\u9ed1': 819, '\u56e0': 820, '\u7b49': 821, '\u5ef6': 822, '\u6751': 823, '\u50b2': 824, '\u5168': 825, '\u5821': 826, '\u804b': 827, '\u5d0e': 828, '\u5176': 829, '\u73bb': 830, '\u74f6': 831, '\u6563': 832, '\u5f04': 833, '\u8bf4': 834, '\u4e3a': 835, 's': 836, '\u9632': 837, '\u5e02': 838, '\u5269': 839, '\u6d77': 840, 'w': 841, '\u624b': 842, '\u6eda': 843, '\u8fd9': 844, '\u672c': 845, '\u8981': 846, '\u51b7': 847, '\u8fdc': 848, '\u53ed': 849, '\u9f41': 850, '\u4e5f': 851, 'l': 852, '\u4fbf': 853, '\u6cea': 854, '\u4f3d': 855, '\u677f': 856, '\u6bef': 857, '\u5578': 858, '\u56a3': 859, '\u6c89': 860, '\u53c2': 861, '\u5410': 862, '\u90fd': 863, '\u63a9': 864, '\u5341': 865, '\u95ee': 866, '\u5f62': 867, '\u914b': 868, '\u7435': 869, '\u54df': 870, '\u9762': 871, '\u9886': 872, '\u76fc': 873, '\u5e26': 874, '\u683c': 875, '\u5c06': 876, '\u4e70': 877, '\u79cd': 878, '\u75b2': 879, '\u5237': 880, '\u5411': 881, '\u8c8c': 882, '\u5f1f': 883, '\u70db': 884, '\u5077': 885, '\u597d': 886, '\u4e09': 887, ']': 888, '\u6ce3': 889, '\u8457': 890, '\u6811': 891, '\u90ca': 892, '\u54a7': 893, '\u4f55': 894, '\u6296': 895, '\u8ba9': 896, '\u9ad8': 897, '\u9041': 898, '\u7a7a': 899, '\u59fb': 900, '\u89e3': 901, '\u813e': 902, '\u8fd0': 903, '\u7eea': 904, '\u6001': 905, '\\u3000': 906, '\u7fd4': 907, '\u6775': 908, '\u5916': 909, '\u5ff5': 910, '\u5e7b': 911, 'u': 912, '\u533a': 913, '\u9f99': 914, '\u52a0': 915, '\u5883': 916, '\u7a97': 917, '\u67aa': 918, '\u6559': 919, '\u4eb2': 920, '\u629b': 921, '\u90a3': 922, '\u672a': 923, '\u6599': 924, '\u6643': 925, '\u5462': 926, '\u95f2': 927, '\u591c': 928, '\u4e3b': 929, 'c': 930, '\u56db': 931, '\u5468': 932, '\u4fe1': 933, '\u5b87': 934, '\u9675': 935, '\u9635': 936, '\u5b64': 937, '\u5bf9': 938, '\u5c42': 939, '\u98de': 940, '\u529e': 941, '\u8eb2': 942, '\u65a4': 943, '\u7470': 944, '\u6447': 945, '\u8ba4': 946, '\u8a8c': 947, 'f': 948, '\u4e13': 949, '\u6012': 950, '\u5385': 951, '\u987a': 952, '\u5e03': 953, '\u53ea': 954, '\u5a18': 955, '\u6750': 956, '\u5543': 957, '\u89c1': 958, '\u4eae': 959, '\u5ea6': 960, '\u7070': 961, '\u5144': 962, '\u5f39': 963, '\u80af': 964, '\u5507': 965, '\u7b1b': 966, '\u675c': 967, '\u9ac5': 968, '\u62e5': 969, '\u7434': 970, '\u684c': 971, '\u54ea': 972, '\u5e95': 973, '\u904d': 974, '\u4e22': 975, '\u6bdb': 976, '\u6280': 977, '\u914d': 978, '\u4e58': 979, '\u8d25': 980, '\u53e5': 981, '\u952e': 982, '\u5466': 983, '\u665a': 984, '\u5347': 985, '\u7247': 986, '\u788e': 987, '\u6768': 988, '\u7ba1': 989, '\u6765': 990, '\u7ec8': 991, '\u5496': 992, '\u7ed5': 993, '\u4f1a': 994, '\u9879': 995, '\u65b0': 996, '\u8361': 997, '\u5566': 998, '\u9057': 999, '\u5426': 1000, '\u5bc4': 1001, '\u7f8a': 1002, '\u5938': 1003, '\u6295': 1004, '\u9664': 1005, '\u4e91': 1006, '\u627f': 1007, '\u5988': 1008, '\u70ba': 1009, '\u7740': 1010, '\u9677': 1011, '\u679c': 1012, '\u5230': 1013, '\u62a4': 1014, '\u888b': 1015, '\u6210': 1016, '\u88c5': 1017, '\u624d': 1018, '?': 1019, '\u9a73': 1020, '\u5df7': 1021, '\u5b81': 1022, '\u8679': 1023, '\u8dd1': 1024, '\u6cfd': 1025, '\u8a00': 1026, '\u517d': 1027, '\u5165': 1028, 'j': 1029, '\u9012': 1030, '\u6307': 1031, '\u793c': 1032, '\u86c7': 1033, '\u654c': 1034, '\u51c9': 1035, '\u504f': 1036, '\u5e74': 1037, '\u4e86': 1038, '\u73af': 1039, '\u5f80': 1040, '\u699c': 1041, '\u730e': 1042, '\u91cf': 1043, '\u7ffb': 1044, '\u4f60': 1045, '\u86ee': 1046, '\u58f0': 1047, '\u5b66': 1048, '\u900f': 1049, '\u6c88': 1050, '\u8001': 1051, '\u5427': 1052, '\u6bb5': 1053, '\u9605': 1054, '\u62cb': 1055, '\u5f8b': 1056, '\u7537': 1057, '\u56fd': 1058, '\u7897': 1059, '\u8ddd': 1060, '\u5dee': 1061, '\u5524': 1062, '\u6e56': 1063, 'k': 1064, '\u65e7': 1065, '\u6570': 1066, '\u62a2': 1067, '\u9192': 1068, '\u751c': 1069, '\u6084': 1070, '\u8840': 1071, '\u5f85': 1072, '\u6027': 1073, '\u6a21': 1074, '\u5343': 1075, '\u72b6': 1076, '\u6298': 1077, '\u53cb': 1078, '\u6b21': 1079, '\u501f': 1080, '\u719f': 1081, '\u503e': 1082, '\u5dfe': 1083, '\u6b77': 1084, '\u5979': 1085, '\u613f': 1086, '\u4e3d': 1087, 'e': 1088, '\u5371': 1089, '\u8272': 1090, '\u7231': 1091, '\u51b3': 1092, '\u559d': 1093, '\u695a': 1094, '\u996e': 1095, '\u95f7': 1096, '\u61c2': 1097, '\u94c1': 1098, '\u79cb': 1099, '\u53f3': 1100, '\u62cd': 1101, '\u5bfc': 1102, '\u63d0': 1103, '\u4e49': 1104, '\u6f20': 1105, '\u8bcd': 1106, 'r': 1107, '\u83b1': 1108, '\u9700': 1109, '\u77e5': 1110, '\u4f4d': 1111, '\u72fc': 1112, '\u56fe': 1113, '\u5766': 1114, '\u6ab3': 1115, '\u4fa0': 1116, '\u5c3d': 1117, '\u5973': 1118, '\u971c': 1119, '\u7f8e': 1120, '\u52c9': 1121, '\u5b58': 1122, '\u876a': 1123, '\u62ec': 1124, '\u539f': 1125, '\u82f1': 1126, '\u6839': 1127, 'i': 1128, '\u90ae': 1129, '\u8c37': 1130, '\u7535': 1131, '\u620f': 1132, '\u8131': 1133, '\u56f4': 1134, '\u5409': 1135, '\u72f0': 1136, '\u8bdd': 1137, '\u4e4b': 1138, '\u786c': 1139, '\u6cb3': 1140, '\u54ed': 1141, '\u5bc2': 1142, '\u627e': 1143, '\u9ab7': 1144, '\u8bb2': 1145, '\u66b4': 1146, '\u505a': 1147, ' ': 1148, '\u8352': 1149, '\u81ea': 1150, '\u6ce5': 1151, '\u636e': 1152, '\u592a': 1153, '\u53c8': 1154, '\u52b3': 1155, '\u65ad': 1156, 'g': 1157, '\u706f': 1158, '\u5b50': 1159}\n"
 }
]
```

然后可以把每个字符转成从0开始的指数(index)来方便之后的使用。

```{.python .input  n=4}
corpus_indices = [char_to_idx[char] for char in corpus_chars]

sample = corpus_indices[:40]

print('chars: \n', ''.join([idx_to_char[idx] for idx in sample]))
print('\nindices: \n', sample)
```

```{.json .output n=4}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "chars: \n \u9418\u9006\u65f6\u9418\u800c\u7ed5 \u6076\u7269\u72f0\u72de\u7684\u503e\u5de2\u6211\u8c26\u5351\u5b89\u9759\u7684\u65bc\u57ce\u5821\u4e0b\u7684\u665a\u7977\u538b\u6291\u8fdc\u53e4\u6d41\u7a9c\u7684\u86ee\u8352\u6697\u53f7\u800c\u7ba1\n\nindices: \n [600, 424, 351, 600, 80, 993, 1148, 501, 315, 1136, 206, 640, 1082, 153, 453, 39, 230, 506, 758, 640, 594, 224, 826, 587, 640, 984, 157, 161, 60, 848, 123, 167, 446, 640, 1046, 1149, 763, 395, 80, 989]\n"
 }
]
```

## 数据读取

同前一样我们需要每次随机读取一些（`batch_size`个）样本和其对用的标号。这里的样本跟前面有点不一样，这里一个样本通常包含一系列连续的字符（前馈神经网络里可能每个字符作为一个样本）。

如果我们把序列长度（`seq_len`）设成10，那么一个可能的样本是`The Time T`。其对应的标号仍然是长为10的序列，每个字符是对应的样本里字符的后面那个。例如前面样本的标号就是`he Time Tr`。

下面代码每次从数据里随机采样一个批量：

```{.python .input  n=5}
import random
from mxnet import nd

def data_iter(batch_size, seq_len, ctx=None):
    num_examples = (len(corpus_indices)-1) // seq_len
    num_batches = num_examples // batch_size
    # 随机化样本
    example_indices = list(range(num_examples))
    random.shuffle(example_indices)

    # 返回seq_len个数据
    def _data(pos):
        return corpus_indices[pos: pos + seq_len]

    for i in range(num_batches):
        # 每次读取batch_size个随机样本
        i = i * batch_size
        batch_indices = example_indices[i: i + batch_size]
        data = nd.array(
            [_data(j * seq_len) for j in batch_indices], ctx=ctx)
        label = nd.array(
            [_data(j * seq_len + 1) for j in batch_indices], ctx=ctx)
        yield data, label
```

看下读出来长什么样：

```{.python .input  n=6}
for data, label in data_iter(batch_size=3, seq_len=8):
    print('data: ', data, '\n\nlabel:', label)
    break
```

```{.json .output n=6}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "data:  \n[[  359.   106.   990.   246.   640.    54.   231.   750.]\n [  558.   452.    31.  1146.   303.   841.   912.    27.]\n [  514.   337.   433.   535.   886.  1108.   258.   640.]]\n<NDArray 3x8 @cpu(0)> \n\nlabel: \n[[  106.   990.   246.   640.    54.   231.   750.     2.]\n [  452.    31.  1146.   303.   841.   912.    27.  1148.]\n [  337.   433.   535.   886.  1108.   258.   640.   174.]]\n<NDArray 3x8 @cpu(0)>\n"
 }
]
```

## 循环神经网络

在对输入输出数据有了解后，我们来正式介绍循环神经网络。

首先回忆下单隐层的前馈神经网络的定义，假设隐层的激活函数是$\phi$，那么这个隐层的输出就是

$$H = \phi(X W_{wh} + b_h)$$

最终的输出是

$$\hat{Y} = \text{softmax}(H W_{hy} + b_y)$$

（跟[多层感知机](../chapter_multilayer-neural-network/mlp-scratch.md)相比，这里我们把下标从$W_1$和$W_2$改成了意义更加明确的$W_{wh}$和$W_{hy}$)

将上面网络改成循环神经网络，我们首先对输入输出加上时间戳$t$。假设$X_t$是序列中的第$t$个输入，对应的隐层输出和最终输出是$H_t$和$\hat{Y}_t$。循环神经网络只需要在计算隐层的输出的时候加上跟前一时间输入的加权和，为此我们引入一个新的可学习的权重$W_{hh}$：

$$H_t = \phi(X_t  W_{xh} + H_{t-1} W_{hh} + b_h )$$

输出的计算跟前一致：

$$\hat{Y}_t = \text{softmax}(H_t W_{hy} + b_y)$$

一开始我们提到过，隐层输出（又叫隐藏状态）可以认为是这个网络的记忆。它存储前面时间里面的信息。我们的输出是完全只基于这个状态。最开始的状态，$H_{-1}$，通常会被初始为0.

## Onehot编码

注意到每个字符现在是用一个整数来表示，而输入进网络我们需要一个定长的向量。一个常用的办法是使用onehot来将其表示成向量。就是说，如果值是$i$, 那么我们创建一个全0的长为`vocab_size`的向量，并将其第$i$位表示成1.

```{.python .input  n=7}
nd.one_hot(nd.array([0,4]), vocab_size)
```

```{.json .output n=7}
[
 {
  "data": {
   "text/plain": "\n[[ 1.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n<NDArray 2x1160 @cpu(0)>"
  },
  "execution_count": 7,
  "metadata": {},
  "output_type": "execute_result"
 }
]
```

记得前面我们每次得到的数据是一个`batch_size x seq_len`的批量。下面这个函数将其转换成`seq_len`个可以输入进网络的`batch_size x vocba_size`的矩阵

```{.python .input  n=8}
def get_inputs(data):
    return [nd.one_hot(X, vocab_size) for X in data.T]

inputs = get_inputs(data)
print('input length: ',len(inputs))
print('input[0] shape: ', inputs[0].shape)
```

```{.json .output n=8}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "input length:  8\ninput[0] shape:  (3, 1160)\n"
 }
]
```

## 初始化模型参数

模型的输入和输出维度都是`vocab_size`。

```{.python .input  n=9}
import mxnet as mx

# 尝试使用 GPU
import sys
sys.path.append('..')
import utils
ctx = utils.try_gpu()
print('Will use ', ctx)

num_hidden = 256
weight_scale = .01

# 隐含层
Wxh = nd.random_normal(shape=(vocab_size,num_hidden), ctx=ctx) * weight_scale
Whh = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx) * weight_scale
bh = nd.zeros(num_hidden, ctx=ctx)
# 输出层
Why = nd.random_normal(shape=(num_hidden,vocab_size), ctx=ctx) * weight_scale
by = nd.zeros(vocab_size, ctx=ctx)

params = [Wxh, Whh, bh, Why, by]
for param in params:
    param.attach_grad()
```

```{.json .output n=9}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Will use  cpu(0)\n"
 }
]
```

## 定义模型

我们将前面的模型公式定义直接写成代码。

```{.python .input  n=10}
def rnn(inputs, H):
    # inputs: seq_len 个 batch_size x vocab_size 矩阵
    # H: batch_size x num_hidden 矩阵
    # outputs: seq_len 个 batch_size x vocab_size 矩阵
    outputs = []
    for X in inputs:
        H = nd.tanh(nd.dot(X, Wxh) + nd.dot(H, Whh) + bh)
        Y = nd.dot(H, Why) + by
        outputs.append(Y)
    return (outputs, H)
```

做个简单的测试：

```{.python .input  n=11}
state = nd.zeros(shape=(data.shape[0], num_hidden), ctx=ctx)
outputs, state_new = rnn(get_inputs(data.as_in_context(ctx)), state)

print('output length: ',len(outputs))
print('output[0] shape: ', outputs[0].shape)
print('state shape: ', state_new.shape)
```

```{.json .output n=11}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "output length:  8\noutput[0] shape:  (3, 1160)\nstate shape:  (3, 256)\n"
 }
]
```

## 预测序列

在做预测时我们只需要给定时间0的输入和起始隐藏状态。然后我们每次将上一个时间的输出作为下一个时间的输入。

![](../img/rnn_3.png)

```{.python .input  n=12}
def predict(prefix, num_chars):
    # 预测以 prefix 开始的接下来的 num_chars 个字符
    prefix = prefix.lower()
    state = nd.zeros(shape=(1, num_hidden), ctx=ctx)
    output = [char_to_idx[prefix[0]]]
    for i in range(num_chars+len(prefix)):
        X = nd.array([output[-1]], ctx=ctx)
        Y, state = rnn(get_inputs(X), state)
        #print(Y)
        if i < len(prefix)-1:
            next_input = char_to_idx[prefix[i+1]]
        else:
            next_input = int(Y[0].argmax(axis=1).asscalar())
        output.append(next_input)
    return ''.join([idx_to_char[i] for i in output])
```

## 梯度剪裁

在求梯度时，循环神经网络因为需要反复做`O(seq_len)`次乘法，有可能会有数值稳定性问题。（想想 $2^{40}$和$0.5^{40}$）。一个常用的做法是如果梯度特别大，那么就投影到一个比较小的尺度上。假设我们把所有梯度接成一个向量 $\boldsymbol{g}$，假设剪裁的阈值是$\theta$，那么我们这样剪裁使得$\|\boldsymbol{g}\|$不会超过$\theta$：

$$ \boldsymbol{g} = \min\left(\frac{\theta}{\|\boldsymbol{g}\|}, 1\right)\boldsymbol{g}$$

```{.python .input  n=13}
def grad_clipping(params, theta):
    norm = nd.array([0.0], ctx)
    for p in params:
        norm += nd.sum(p.grad ** 2)
    norm = nd.sqrt(norm).asscalar()
    if norm > theta:
        for p in params:
            p.grad[:] *= theta/norm
```

## 训练模型

下面我们可以还是训练模型。跟前面前置网络的教程比，这里只有两个不同。

1. 通常我们使用Perplexit(PPL)这个指标。可以简单的认为就是对交叉熵做exp运算使得数值更好读。
2. 在更新前我们对梯度做剪裁

```{.python .input  n=14}
#seq1 = 'The Time Ma'
#seq2 = "The Medical Man rose, came to the lamp,"
seq1 = '为什么'
seq2 = "为什么这样子"

from mxnet import autograd
from mxnet import gluon
from math import exp

epochs = 200
seq_len = 35
learning_rate = .1
batch_size = 32

softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()

for e in range(epochs+1):
    train_loss, num_examples = 0, 0
    state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)
    for data, label in data_iter(batch_size, seq_len, ctx):
        with autograd.record():
            outputs, state = rnn(get_inputs(data), state)
            # reshape label to (batch_size*seq_len, )
            # concate outputs to (batch_size*seq_len, vocab_size)
            label = label.T.reshape((-1,))
            outputs = nd.concat(*outputs, dim=0)
            loss = softmax_cross_entropy(outputs, label)
        loss.backward()

        grad_clipping(params, 5)
        utils.SGD(params, learning_rate)

        train_loss += nd.sum(loss).asscalar()
        num_examples += loss.size

    if e % 20 == 0:
        print("Epoch %d. PPL %f" % (e, exp(train_loss/num_examples)))
        print(' - ', predict(seq1, 100))
        print(' - ', predict(seq2, 100), '\n')

```

```{.json .output n=14}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "Epoch 0. PPL 967.232933\n -  \u4e3a\u4ec0\u4e48                                                                                                     \n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50                                                                                                      \n\nEpoch 20. PPL 403.729662\n -  \u4e3a\u4ec0\u4e48\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684\u7684 \n\nEpoch 40. PPL 360.244198\n -  \u4e3a\u4ec0\u4e48\u7684 \u6211\u7684 \u6211\u4e0d\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50\u7684 \u6211\u7684 \u6211\u4e0d\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef\u6211\u7684\u6211\u7684\u53ef\u7684\u8ba9\u6211\u7684\u53ef\u7684\u53ef \n\nEpoch 60. PPL 250.150746\n -  \u4e3a\u4ec0\u4e48 \u4f60\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50 (\u4e0d\u4e86 \u6211\u4e0d\u662f \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef\u7231 \u6211\u4e0d\u4e86 \u6211\u7684\u8ba9\u6211 \u6211\u7684\u53ef \n\nEpoch 80. PPL 174.783956\n -  \u4e3a\u4ec0\u4e48 \u4f60\u4e0d\u8981\u6211\u4e0d\u8981\u6211\u8bf4\u4f60\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50 \u6211\u4e0d\u8981\u4f60\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d\u8981\u6211\u4e0d\u60f3 \u4f60\u4e0d \n\nEpoch 100. PPL 121.415789\n -  \u4e3a\u4ec0\u4e48 (\u70ba\u4e86\u6211\u7684\u6eaa\u8fb9 \u8c01\u900f (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50 \u6211\u4e0d\u56de \u6211\u60f3\u8981\u4f60\u7684\u6eaa\u8fb9 \u8c01\u9ed8 (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684\u4e50\u95e8 \u4e09\u53eb (\u4f60\u7684\u4f60 \u4f60\u7684 \n\nEpoch 120. PPL 90.458877\n -  \u4e3a\u4ec0\u4e48 (\u70ba\u6211\u7684\u88c5\u7b11 \u8c01\u53eb\u6211\u4e0d\u60f3\u8981\u4f60\u8bf4\u6ca1\u6709\u4e00\u4e2a (\u5728\u4e00\u4e00\u4e2a\u620f \u4e09\u70ba\u4e86\u4e00\u8d77\u4e24\u9897\u4e09\u9897\u56db\u9897 \u4f60\u7684\u53ef\u7231\u5973\u4eba\u6f02\u4eae\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50 (\u70ba\u6211\u7684\u4f60\u7b11\u4f60\u8bf4\u4e48 \u4f60\u7684\u53ef\u7231\u4f60\u7684\u5b8c\u60c5 \u4e09\u5973\u4eba\u662f\u6211\u7684\u5144\u5f1f \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f  \n\nEpoch 140. PPL 64.351252\n -  \u4e3a\u4ec0\u4e48 (\u70ba\u4e00\u7897\u70ed\u7ca5 \u4e00e\u4eba \u4f60\u5728\u4e86 (\u624b\u4e86 \u662f\u6211\u7684m\u4e3d \u6211\u6e05\u4ed6\u6709 \u6211\u60f3\u8981\u4f60\u7684\u6eaa\u8fb9 \u8036\u9ed8\u4e00\u4e48 (\u7684\u53ef\u7231\u5973\u4eba\u6f02\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50\u4e00\u4e2a\u4e24\u67f3\u5728\u6b65\u56db\u6b65\u6211 \u6cea\u4e0d\u4f11 \u8bed\u6c89\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684 \n\nEpoch 160. PPL 47.870864\n -  \u4e3a\u4ec0\u4e48 (\u4e0d\u8981\u6211\u7684\u6eaa\u8fb9\u6211\u8bf4\u5e97\u5c0f \u6211\u60f3\u8981\u4f60\u6211\u60f3\u8981\u4f60 \u554a\u4f0a\u53bb\u7684\u98ce\u6001 \u70ba\u4f60\u7684\u9ed1\u4e3d.\u7684\u5b8c\u7f8e \u6211\u5f00\u7740\u662f\u6211\u542c\u4f60\u8bf4\u554a\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50 \u6211\u4e0d\u56de \u6211\u6c89\u5f88\u8fd9\u6e38\u624b \u4f60\u4e0d\u60f3\u8981\u591a\u6211\u60f3\u8981 \u4f60\u60f3\u5f88\u4e45\u6211\u7684\u7f8e\u60c5 \u6211\u60f3\u8981\u6709 \u6211\u4e0d\u8981\u6211\u7684\u8bc1\u8fb9 \u554a\u9ed8\u4e00\u6574 (\u7684\u53ef\u7231\u5973\u4eba\u6f02\u4eae\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973 \n\nEpoch 180. PPL 37.399144\n -  \u4e3a\u4ec0\u4e48 (\u662f\u6211\u4e0d\u60f3\u8981\u8bf4 \u5206\u661f\u662f \u4f60\u5728\u4e86\u6709\u4e9b \u7231\u4e0a\u4e00\u7897\u70ed\u7ca5 \u914d\u6211\u7684mr\u4f60\u7684\u5b8c\u7f8e\u4e3b\u4e49 \u592a\u5f7b\u5e95\u5fd8\u624b\u6211\u60f3\u60f3\u4f60\u7684\u5fae\u7b11 \u8c01\u53eb\u4ed6\u5916 \u6d6a\u624b\u64cd\u5fc3\u867d\u7136\u4e0d\u7740\u6211\u7684\u5fae\u60c5 \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8036\u53eb\u4ed6\u4e48 \u8fde\u7136\u64cd\u5fc3\u867d\u7136\u4e0d\u7740\u6211\u7684\u4e8b\u60c5 \u8c01\u53eb\u4ed6\u662f\u6211\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50\u53ea \u6211\u7684\u53ef\u7231\u5973\u4eba\u900f\u660e\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef\u7231\u5973\u4eba\u574f\u574f\u7684\u8ba9\u6211\u75af\u72c2\u7684\u53ef \n\nEpoch 200. PPL 30.762860\n -  \u4e3a\u4ec0\u4e48\u6211\u60f3\u8981\u61c2\u4f60\u6211\u4e0d\u80fd\u61c2\u7740\u6211\u4e0d\u80fd \u4f60\u60f3\u5f88\u4e45\u4e86\u5427?\u6211\u4e0d\u60f3\u8981\u88ab\u6211\u770b\u7f8e\u4f60\u662f\u4f60\u7684\u4e8b\u60c5 \u8c01\u53eb\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8036\u62e2\u4ed6\u4e48\u6211\u7684\u5144\u5f1f \u8036\u62e2\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8036\u62e2\u4ed6\u4e48\u6211\u7684\u5144\u5f1f \u8036\u62e2\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8036\u62e2\u4ed6\u4e48\u6211\u7684\u5144\u5f1f \u8036\u62e2\u4ed6\u662f\u6211\u7684\u5144\u5f1f \u8036\u62e2\n -  \u4e3a\u4ec0\u4e48\u8fd9\u6837\u5b50\u4e00\u4e2a\u878d\u8868\u6211\u4f1a\u6cea\u610f\u88ab\u6211\u770b\u89c1\u4f60\u4e0d\u60f3\u8981\u4e45\u4e86 \u53ea\u662f\u6211\u4f1a\u4e0d\u5230\u82e5\u901f\u5ea6\u79cb (\u5929\u9ec4\u7684\u4f60\u6001\u7434\u70ba\u6c38\u5929 \u4e34\u6545\u4e00\u5c0f\u624b\u7ca5 \u4e00\u8fb9\u4e00\u7897\u7684\u6eaa\u66f2\u6211\u8bf4\u5e97\u5c0f\u4e8c \u4e09\u4e24\u94f6\u591f\u4e0d\u591f\u666f\u597d\u5165\u79cb \u6f2b\u5929\u9ec4\u7684\u59ff\u6001\u7434\u70ba \u9192\u53c8\u4e00\u7897 (\u624b\u4e00\u6b21 (\u8bb0\u4e00\u7897\u597d\u7ca5 \u914d\u4e0a\u4e00 \n\n"
 }
]
```

可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。

## 结论

通过隐藏状态，循环神经网络很够更好的使用数据里的时序信息。

## 练习

调调参数（数据集大小，模型复杂度，学习率），看看对Perplexity和预测的结果造成的区别。

**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/989)
