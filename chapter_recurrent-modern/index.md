# 现代复发神经网络
:label:`chap_modern_rnn`

我们介绍了 RNS 的基础知识，可以更好地处理序列数据。为了演示，我们在文本数据上实施了基于 RNN 的语言模型。然而，如今从业人员面临广泛的序列学习问题，这些技术对他们来说可能是不够的。

例如，在实践中一个值得注意的问题是 RNs 的数值不稳定性。虽然我们已经应用了实现技巧，如梯度裁剪，但通过更复杂的序列模型设计可以进一步缓解这个问题。具体而言，门控 RNN 在实践中更为常见。我们将首先介绍其中两个广泛使用的网络，即 * 门控循环单元 * (GRU) 和 * 长短期内存 * (LSTM)。此外，我们将扩展 RNN 架构，使用迄今已讨论过的单一无向隐藏层。我们将描述具有多个隐藏层的深层架构，并讨论采用前向和后向循环计算的双向设计。在现代经常性网络中，这种扩展经常被采用。在解释这些 RNN 变体时，我们将继续考虑 :numref:`chap_rnn` 中引入的相同语言建模问题。

事实上，语言建模只能揭示序列学习能力的一小部分。在各种序列学习问题（如自动语音识别、文本到语音和机器翻译）中，输入和输出都是任意长度的序列。为了解释如何拟合这种类型的数据，我们将以机器翻译为例，并介绍基于 RNS 和光束搜索的编码器-解码器架构，用于序列生成。

```toc
:maxdepth: 2

gru
lstm
deep-rnn
bi-rnn
machine-translation-and-dataset
encoder-decoder
seq2seq
beam-search
```
