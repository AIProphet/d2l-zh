# 束搜索

上一节介绍了如何训练输入输出均为不定长序列的编码器—解码器，这一节我们介绍如何使用编码器—解码器来预测不定长的序列。

上一节里已经提到，在准备训练数据集时，我们通常会在样本的输入序列和输出序列后面分别附上一个特殊符号“&lt;eos&gt;”表示序列的终止。我们在接下来的讨论中也将沿用上一节的数学符号。为了便于讨论，假设解码器的输出是一段文本序列。设输出文本词典$\mathcal{Y}$（包含特殊符号“&lt;eos&gt;”）的大小为$|\mathcal{Y}|$，输出序列的最大长度为$T'$。所有可能的输出序列一共有$\mathcal{O}(|\mathcal{Y}|^{T'})$种。这些输出序列中所有特殊符号“&lt;eos&gt;”及其后面的子序列将被舍弃。



## 穷举搜索

我们在描述解码器时提到，输出序列基于输入序列的条件概率是$\prod_{t'=1}^{T'} \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c})$。为了搜索该概率最大的输出序列，一种方法是穷举所有可能序列的概率，并输出概率最大的序列。我们将该序列称为最优序列，并将这种搜索方法称为穷举搜索（exhaustive search）。

虽然穷举搜索可以得到最优的预测序列，但它的计算开销$\mathcal{O}(|\mathcal{Y}|^{T'})$很容易过大。例如，当$|\mathcal{Y}|=10000$且$T'=10$时，我们将评估$10000^{10} = 10^{40}$个序列：这几乎不可能完成。



## 贪婪搜索

我们还可以使用贪婪搜索（greedy search）。也就是说，对于输出序列任一时间步$t'$，从$|\mathcal{Y}|$个词中搜索出输出词

$$y_{t'} = \text{argmax}_{y_{t'} \in \mathcal{Y}} \mathbb{P}(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c}),$$

且一旦搜索出“&lt;eos&gt;”符号即完成输出。

下面我们来看一个例子。假设输出词典里面有“A”，“B”，“C”和“&lt;eos&gt;”这四个词。图10.3中每个时间步下的四个数字分别代表了该时间步生成“A”，“B”，“C”和“&lt;eos&gt;”这四个词的条件概率。在每个时间步，贪婪搜索选取生成的条件概率最高的词。因此，图10.3中将生成序列“A”、“B”和“C”（舍弃特殊符号“&lt;eos&gt;”）。该输出序列的条件概率是$0.5\times0.4\times0.4 = 0.08$。


![每个时间步下的四个数字分别代表了该时间步生成“A”，“B”，“C”和“&lt;eos&gt;”这四个词的条件概率。在每个时间步，贪婪搜索选取生成的条件概率最高的词。](../img/s2s_prob1.svg)


正如绝大部分贪婪算法不能保证最优解一样，贪婪搜索也无法保证找出条件概率最高的输出序列。图10.4演示了这样的一个例子。与图10.3中不同，图10.4在时间步2中选取了条件概率第二大的“C”。由于时间步3所基于的时间步1和2的输出子序列由图10.3的“A”和“B”变为了图10.4的“A”和“C”，图10.4中时间步3生成各个词的条件概率发生了变化。我们选取条件概率最大的“B”。此时时间步4所基于的前三个时间步的输出子序列为“A”、“C”和“B”。图10.4中时间步4生成各个词的条件概率也与图10.3中的不同。我们发现，此时的输出序列“A”、“C”和“B”的条件概率是$0.5\times0.3\times0.6=0.09$，高于贪婪搜索得到的输出序列的条件概率。

![每个时间步下的四个数字分别代表了该时间步生成“A”，“B”，“C”和“&lt;eos&gt;”这四个词的条件概率。在时间步2选取条件概率第二大的“C”。](../img/s2s_prob2.svg)



## 束搜索

让我们再来回顾下贪婪搜索和穷举搜索。它们可以概况成如下的算法。假设在每个时间步$t'$我们现在有$n$条长度为$t'$的候选输出序列（包括了开始符）。例如当$t'=0$时就是一条只含有开始符的序列。然后对输出字典里每个候选词计算条件概率，这样我们可以得到$n|\mathcal{O}|$条长度为$t'+1$的候选输出序列和它们的条件概率，然后我们在其中筛选出进入下一个时间步的候选序列。

前面两个搜索算法不同的地方就在于筛选这一步。贪婪搜索只保留概率最高的序列进入下一时间步，而穷举搜索则保留所有。前者计算简单，其计算复杂度为$\mathcal{O}(|T'|)$，但难以保证输出质量。后者保证最优输出，但计算复杂度高达$\mathcal{O}(|\mathcal{Y}|^{T'})$，其实际中几乎不可能完成。

束搜索（beam search）介于两者之间，在筛选时它只保留条件概率最高的$k$条序列，这里$k$叫做束宽（beam size），是一个超参数。当$k=1$时其等价于贪婪搜索，而$k=\infty$时其等价于穷举搜索。束搜索的算法复杂度为$\mathcal{O}(kT'|\mathcal{Y}|)$。实际使用中，我们通过$k$来权衡输出序列质量和计算复杂度。下图演示了束搜索的工作原理。

![束宽为2的束搜索，每个时间步选取条件概率最高的两个序列作为候选进入下一时间。](../img/beam_search.svg)

束搜索的停止条件有多种，例如找到一条有终止符的序列，或者找到一条条件概率高于某个阈值的序列，或者到达了最大输出长度。停止时束搜索输出最多$k$个候选输出序列。如果某个序列中含有终止符，那么去掉终止符后面的子序列。通常我们保留最佳的一个或数个序列作为最终输出。

由于输出序列可能长度不一样，较短的序列通常条件概率比较大。在比较时经常将长度信息考虑在内。例如对于长为$L$的序列$y_1,\ldots,y_L$，我们将其对数条件概率除以$L^\alpha$作为分数，然后选取分数高的作为最终输出。这里分数的计算为

$$ \frac{1}{L^\alpha} \log \mathbb{P}(y_1, \ldots, y_{L}\mid \mathrm{context}) = \frac{1}{L^\alpha} \sum_{t^\prime=1}^L \log \mathbb{P}(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \mathrm{context}),$$

常数$\alpha$一般可选为0.75。


## 小结

* 预测不定长序列的方法包括穷举搜索、贪婪搜索和束搜索。
* 束搜索通过更灵活的束宽来权衡计算开销和搜索质量。


## 练习

* 在[“循环神经网络”](../chapter_recurrent-neural-networks/rnn.md)一节中，我们使用语言模型创作歌词。它的输出属于哪种搜索？你能改进它吗？


## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/6817)

![](../img/qr_beam-search.svg)

## 参考文献

[1] Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112).
